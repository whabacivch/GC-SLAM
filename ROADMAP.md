# FL-SLAM Roadmap (Impact Project v1)

This roadmap is organized around the current **M3DGR rosbag MVP** pipeline and a clean separation between:
- **MVP operational code**: required to run `tools/run_and_evaluate.sh`
- **Near-term priorities**: Wheel odom separation, dense RGB-D in 3D mode, evaluation hardening
- **Future/experimental code**: kept for later work, but not required for the MVP

---

## âœ… Completed Milestones

### IMU Integration & 15D State Extension (2026-01-21)
- **Contract B IMU Fusion**: Frontend publishes raw IMU segments (`/sim/imu_segment`), backend re-integrates with sigma-point propagation and bias coupling
- **15D State Extension**: Backend maintains 15DOF state (pose + velocity + biases) per anchor module
- **Two-State Schur Marginalization**: Joint e-projection followed by exact Schur marginalization
- **Hellinger-Dirichlet Fusion**: Batched moment matching with Hellinger-tilted likelihood and Dirichlet-categorical routing
- **Files**: `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/imu_jax_kernel.py`, `dirichlet_routing.py`, `lie_jax.py`, `gaussian_info.py`
- **Message**: `IMUSegment.msg` (replaces deprecated `IMUFactor.msg`)

### Package Structure Flattening (2026-01-22)
- Flattened package structure: `common/`, `frontend/`, `backend/` at top level
- Moved utility nodes into `frontend/`
- Updated all imports and entry points

### Existing Adaptive Components
- **Adaptive Process Noise** (`fl_ws/src/fl_slam_poc/fl_slam_poc/backend/process_noise.py`): Inverse-Wishart model for online process noise estimation
- **Adaptive Parameters** (`fl_ws/src/fl_slam_poc/fl_slam_poc/backend/adaptive.py`): Bayesian online parameter estimation with Normal priors
- **Reference**: See `docs/Self-Adaptive Systems Guide.md` for full self-adaptive system specifications

---

## 0) Ground-Truth Facts (M3DGR Dynamic01 Bag) + â€œNo Silent Assumptionsâ€

These are **observed facts from the bag** and should be treated as the default configuration unless explicitly overridden:

- **No `CameraInfo` topics in the bag** â†’ intrinsics must be declared via parameters and logged.
- **No `/tf` or `/tf_static` topics in the bag** â†’ camera extrinsics are not available online unless we provide them (URDF/static transform parameter).
- **Odom frames in the bag**:
  - `world_frame` = `/odom.header.frame_id` = `odom_combined`
  - `base_frame` = `/odom.child_frame_id` = `base_footprint`
- **RGB + aligned depth frames in the bag**:
  - `/camera/color/image_raw/compressed.header.frame_id` = `camera_color_optical_frame`
  - `/camera/aligned_depth_to_color/image_raw/compressedDepth.header.frame_id` = `camera_color_optical_frame`
  - Depth is already registered to color (same pixel grid/frame_id).

**Policy (by construction):**
- Do not â€œguessâ€ frames or extrinsics silently.
- All frame assumptions (world/base/camera frames, intrinsics source, extrinsics source) must be explicit parameters and logged from the first N message headers.

---

## 1) MVP Status (Current Baseline)

**Primary entrypoint**
- `tools/run_and_evaluate.sh`: runs the M3DGR Dynamic01 pipeline end-to-end (SLAM + plots/metrics).

**Launch**
- `fl_ws/src/fl_slam_poc/launch/poc_m3dgr_rosbag.launch.py`

**Nodes in the MVP pipeline**
- Frontend: `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py`
- Backend: `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`
- Utility: `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/image_decompress.py` (moved from `utility_nodes/` during flattening)
- Utility: `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/livox_converter.py` (moved from `utility_nodes/` during flattening)
- Utility: `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/tb3_odom_bridge.py` (moved from `utility_nodes/` during flattening; generic absâ†’delta odom bridge, legacy name)

**Evaluation**
- `tools/align_ground_truth.py`
- `tools/evaluate_slam.py`

**Current State (15DOF per module)**
```python
mu = [x, y, z, rx, ry, rz, vx, vy, vz, bg_x, bg_y, bg_z, ba_x, ba_y, ba_z]  # 15D: pose + velocity + biases
cov = 15Ã—15 matrix  # [Î´p, Î´Î¸, Î´v, Î´b_g, Î´b_a] in tangent space
```

**Note**: State was extended from 6DOF to 15DOF as part of IMU integration (completed 2026-01-21). See Completed Milestones above.

---

## 2) Near-Term Priority: IMU Integration & 15D State Extension

### Status: âœ… COMPLETED (2026-01-21)

**Implementation**: Contract B IMU fusion architecture with raw IMU segments, sigma-point propagation, and two-state Schur marginalization. See Completed Milestones section above for details.

### Overview

IMU integration has been completed to fix rotation accuracy, extend backend state from 6DOF SE(3) to 15DOF (pose + velocity + biases), and harden multi-sensor fusion. The implementation uses **Contract B architecture**: frontend publishes raw IMU segments (`IMUSegment.msg`), backend re-integrates with sigma-point propagation and bias coupling, then performs joint e-projection followed by exact Schur marginalization.

**Expected improvement:** Rotation RPE should improve ~30-50% with IMU vs. without (wheel odom + LiDAR only).

**Self-Adaptive Systems Integration**: See Section 2.7 below for adaptive IMU noise model integration (future enhancement).

---

### Keyframe Policy (Required Before IMU Factors)

IMU preintegration and â€œtwo-pose factorsâ€ require **explicit keyframes**. Keyframes must be:
- Deterministic and auditable (time-based by default).
- Independent of â€œanchor birthâ€ (which is a probabilistic structure event and not a reliable time axis).

**Keyframe rule (recommended v1):**
- Create keyframe every `keyframe_interval_sec` (default 1.0s).
- Optionally add a motion criterion as a second trigger, but it must be logged as a declared policy.

Everything that produces interval factors (wheel odom, IMU) should be scheduled between consecutive keyframes `(i â†’ i+1)`.

---

### Phase 1: IMU Infrastructure & Preintegration

#### 1.1 IMU Sensor I/O

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/processing/sensor_io.py`

**Changes:**
- Add IMU subscription to `SensorIO` class with configurable topic parameter (`imu_topic`, default `/camera/imu`)
- Add `imu_buffer` to store `(timestamp, accel, gyro)` tuples (high-rate buffering, keep ~1s window)
- Implement `get_imu_measurements(start_sec, end_sec)` to retrieve IMU data for preintegration intervals
- Add duplicate detection for IMU messages (similar to existing `_on_odom`, `_on_image` handlers)
- Add status monitoring registration for IMU sensor

**Invariants preserved:** Pure I/O layer, no math/inference in `SensorIO`; buffering only, no gating.

#### 1.2 IMU Preintegration Operator âœ… COMPLETED

**Files:**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/common/imu_preintegration.py` (moved from `operators/` during flattening)

**Implementation Status:**
- âœ… `IMUPreintegrator` class exists for preintegration utilities
- âœ… Used in Contract B architecture: raw IMU segments published, backend re-integrates in-kernel
- âœ… Reference: Forster et al., "On-Manifold Preintegration for Real-Time Visual-Inertial Odometry" (TRO 2017)

**Note**: Contract B architecture uses raw IMU segments rather than preintegrated factors. Preintegration utilities remain available for reference/utilities.

#### 1.3 IMU Segment Message âœ… COMPLETED

**Files:**
- âœ… `fl_ws/src/fl_slam_poc/msg/IMUSegment.msg` (replaces deprecated `IMUFactor.msg`)
- âœ… `fl_ws/src/fl_slam_poc/CMakeLists.txt` (already includes `IMUSegment.msg`)

**Message**: `IMUSegment.msg` contains raw IMU measurements between keyframes with explicit units/frames/timebase semantics. Backend re-integrates these segments with sigma-point propagation.

#### 1.4 Frontend IMU Segment Publishing âœ… COMPLETED

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py`

**Status:**
- âœ… Publishes `/sim/imu_segment` raw IMU slices at keyframe creation
- âœ… Parameters: `imu_topic`, `imu_gyro_noise_density`, `imu_accel_noise_density`, `imu_gyro_random_walk`, `imu_accel_random_walk`, `enable_imu`
- âœ… Keyframe-based timing (anchor birth remains separate event)

---

### Phase 2: Backend 15D State Extension âœ… COMPLETED

#### 2.1 State Representation Update âœ… COMPLETED

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`

**State (15DOF):**
```python
mu = [x, y, z, rx, ry, rz, vx, vy, vz, bg_x, bg_y, bg_z, ba_x, ba_y, ba_z]
cov = 15Ã—15 matrix  # [Î´p, Î´Î¸, Î´v, Î´b_g, Î´b_a] in tangent space
```

**Status:**
- âœ… `SparseAnchorModule` extended to 15D (velocity=0, biases=0 initialization)
- âœ… `cov` extended to 15Ã—15 with high-variance priors for v, b_g, b_a
- âœ… `make_evidence(mu, cov)` handles 15D information form (L: 15Ã—15, h: 15D)

**Compatibility:**
- âœ… Odometry factors remain 6DOF (update first 6 rows/cols of 15Ã—15 L/h)
- âœ… Loop factors remain 6DOF (update first 6 rows/cols of 15Ã—15 L/h)
- âœ… IMU segments integrated in-kernel with sigma-point propagation (Contract B)
- âœ… Biases [9:15] updated by IMU integration and random walk process noise

#### 2.2 IMU Segment Fusion in Backend âœ… COMPLETED

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`

**Status:**
- âœ… Subscription: `/sim/imu_segment` â†’ `on_imu_segment(msg: IMUSegment)`
- âœ… Contract B architecture: raw IMU segments re-integrated with sigma-point propagation
- âœ… Two-state factor update: joint e-projection on [pose_i, pose_j] (30D) followed by exact Schur marginalization
- âœ… Emits `OpReport` with Contract B diagnostics: `approximation_triggers: {"Linearization"}`, `frobenius_applied: True`

#### 2.3 Information-Form Gaussian Fusion (15D) âœ… COMPLETED

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/gaussian_info.py` (flattened from `backend/fusion/gaussian_info.py`)

**Status:**
- âœ… `make_evidence`, `fuse_info`, `mean_cov` work correctly for 15D inputs
- âœ… `embed_info_form` utility for dimension embedding (6D â†’ 15D)
- âœ… State ordering convention documented: `[p(3), Î¸(3), v(3), b_g(3), b_a(3)]`

---

### Phase 3: Wheel Odometry Separation & Factor Scheduling

#### 3.1 Wheel Odometry as Separate Factor

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`

**Current:** `/sim/odom` is treated as "pose prior" (6DOF delta)

**New approach:**
- Treat wheel odom as a partial observation: constrains `[x, y, yaw]` strongly, `[z, roll, pitch]` weakly (high variance)
- Construct information form using **declared parameters** (no hard-coded constants), with two safe modes:
  1. **Use odom-provided covariance** (preferred when trustworthy): read `nav_msgs/Odometry.pose.covariance`.
  2. **Fallback diagonal covariance** (explicit params): `wheel_sigma_xy`, `wheel_sigma_yaw`, `wheel_sigma_z`, `wheel_sigma_roll`, `wheel_sigma_pitch`.
- Do NOT pre-fuse wheel odom with IMU upstream; keep as independent factor stream

**Rationale:** Wheel odom measures what it measures (XY + yaw), not a full 6DOF pose. Allows IMU to dominate roll/pitch estimation.

**Self-Adaptive Systems Integration**: See `docs/Self-Adaptive Systems Guide.md` Section 2 (Adaptive Sensor Weighting via Fisher Information) for future enhancement: sensor weights should adapt based on Fisher information and reliability scores, replacing fixed covariance with online-estimated Wishart priors.

#### 3.2 Factor Scheduling & Keyframe Policy

**Files:**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py`
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`

**Changes:**
- **Keyframe creation policy (deterministic + auditable):**
  - Time-based: every `keyframe_interval_sec` (default 1.0s)
  - Motion-based (optional): when `||delta_pose|| > keyframe_motion_threshold`
  - Log each keyframe decision: `"Keyframe {id} created at t={t:.3f}s (policy: {reason})"`
- **Factor streams (explicit):**
  - Odometry factors: published between consecutive keyframes (6DOF delta)
  - IMU segments: published between consecutive keyframes (raw IMU measurements, Contract B architecture)
  - Loop factors: published when loop detected (6DOF relative pose)
  - RGB-D factors (optional): published when RGB-D pair available (3DOF position)
- **Backend factor ingestion logging:**
  - Track counts: `n_odom_factors`, `n_imu_factors`, `n_loop_factors`, `n_rgbd_factors`
  - Publish in `/cdwm/backend_status`: `{"factors": {"odom": n, "imu": m, "loop": k, "rgbd": r}}`

---

### Phase 4: Dense RGB-D in 3D Mode (Optional TF)

**Files:** `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py`

**Current:** Dense RGB-D evidence (`_publish_rgbd_evidence`) is only called in 2D LaserScan mode

**Changes:**
- Enable `_publish_rgbd_evidence` in 3D pointcloud mode when `enable_depth=True` and `publish_rgbd_evidence=True`
- Add parameter: `camera_base_extrinsic` (6DOF SE(3) static transform, default None)
- **No-TF mode (documented assumption):**
  - If `camera_base_extrinsic` is provided: use declared static transform (no TF lookup)
  - If `camera_base_extrinsic` is None and TF lookup fails: log warning and skip dense evidence (don't crash)
  - Document in `docs/3D_POINTCLOUD.md`: "RGB-D evidence requires either (1) TF tree or (2) declared camera_base_extrinsic parameter"

---

### Phase 5: Validation & Evaluation Hardening

#### 5.1 Provenance & Contribution Summary

**Files:**
- New: `fl_ws/src/fl_slam_poc/fl_slam_poc/common/provenance.py` (or `backend/provenance.py` if backend-specific)
- Update: `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`, `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py`

**Implementation:**
- Create `provenance.py` module with functions:
  - `get_git_sha()` â†’ current git commit SHA
  - `get_module_path(module_name)` â†’ resolved import path (verifies correct source is loaded)
  - `get_factor_counts()` â†’ dictionary of factor stream counts
- Add provenance logging to backend/frontend node startup (git SHA, module paths, factor streams enabled)
- Add periodic factor count summary (every 10s)

#### 5.2 Baseline Evaluation After IMU Integration

**Files:** `tools/run_and_evaluate.sh`, `tools/evaluate_slam.py`

**Changes:**
- Add rotation metrics (RPE rotation, ATE rotation) to `tools/evaluate_slam.py` (already present, verify)
- Run baseline before/after IMU integration on M3DGR Outdoor01:
  - Baseline (current, no IMU): record ATE/RPE translation + rotation
  - With IMU: record ATE/RPE translation + rotation
  - **Expected:** rotation RPE should improve ~30-50%
- Document results in `CHANGELOG.md` under milestone entry

#### 5.3 Focused Tests

**Files:**
- New: `fl_ws/src/fl_slam_poc/test/test_imu_preintegration.py`
- New: `fl_ws/src/fl_slam_poc/test/test_15d_state.py`

**Test coverage:**
1. **IMU preintegration:** Synthetic IMU data â†’ verify delta_R, OpReport, covariance 9Ã—9 PD
2. **15D state fusion:** Verify odometry factor (6DOF) updates only first 6 dimensions, IMU factor (9DOF) updates first 9 dimensions
3. **Ground truth non-ingestion:** Verify backend never subscribes to `/vrpn_client_node/*` or `/ground_truth` topics

---

### Phase 6: Launch File & Parameter Updates

#### 6.1 M3DGR Launch File

**Files:** `fl_ws/src/fl_slam_poc/launch/poc_m3dgr_rosbag.launch.py`

**Changes:**
- Add IMU parameters:
  ```python
  DeclareLaunchArgument("enable_imu", default_value="true"),
  DeclareLaunchArgument("imu_topic", default_value="/camera/imu"),
  DeclareLaunchArgument("imu_gyro_noise_density", default_value="1.0e-3"),
  DeclareLaunchArgument("imu_accel_noise_density", default_value="1.0e-2"),
  DeclareLaunchArgument("imu_gyro_random_walk", default_value="1.0e-5"),
  DeclareLaunchArgument("imu_accel_random_walk", default_value="1.0e-4"),
  ```

#### 6.2 Alternative Dataset Support (Newer College, etc.)

**Files:** `phase2/fl_ws/src/fl_slam_poc/launch/poc_3d_rosbag.launch.py`

**Changes:**
- Add same IMU parameters as above
- Add override examples in `docs/datasets/DATASET_DOWNLOAD_GUIDE.md`:
  ```bash
  # Newer College (LiDAR IMU)
  ros2 launch fl_slam_poc poc_3d_rosbag.launch.py \
    bag:=rosbags/newer_college/01_short_experiment.bag \
    imu_topic:=/os1_cloud_node/imu \
    pointcloud_topic:=/os1_cloud_node/points
  ```

---

### Milestones & Ordering

**Milestone 1: IMU Sensor + Preintegration (Phase 1) âœ… COMPLETED**
- âœ… IMU I/O, message definition (`IMUSegment.msg`), frontend publishing
- âœ… Contract B architecture: raw IMU segments published to `/sim/imu_segment`
- âœ… **Validation:** M3DGR runs with `enable_imu:=true`, `/sim/imu_segment` messages published
- âœ… **Logs:** OpReport shows `Linearization` trigger + Frobenius correction

**Milestone 2: 15D State + Backend Fusion (Phase 2) âœ… COMPLETED**
- âœ… State representation extended to 15D, Contract B IMU fusion implemented
- âœ… **Validation:** M3DGR runs, backend ingests IMU segments without errors
- âœ… **Logs:** Backend status shows IMU segments processed, Contract B diagnostics present

**Milestone 3: Wheel Odom Separation (Phase 3.1)**
- Update wheel odom to be a partial factor (strong XY/yaw, weak z/roll/pitch)
- **Validation:** Compare trajectory before/after (expect similar XY, improved roll/pitch when fused with IMU)

**Milestone 4: Factor Scheduling Hardening (Phase 3.2)**
- Deterministic keyframe policy + explicit factor stream logging
- **Validation:** Verify factor counts in `/cdwm/backend_status` match expected rates

**Milestone 5: Dense RGB-D in 3D Mode (Phase 4)**
- Enable `_publish_rgbd_evidence` in 3D mode with optional TF
- **Validation:** Run M3DGR with 3D mode + RGB-D evidence, verify no TF crashes

**Milestone 6: Evaluation Hardening (Phase 5)**
- Add provenance logging, run baseline comparison, add focused tests
- **Validation:** Rotation metrics improve ~30-50% with IMU vs. without

---

### Design Invariants Compliance Checklist

- **Frobenius correction:** IMU preintegration emits `approximation_triggers: {"Linearization"}` and applies third-order correction to covariance
- **Closed-form-first:** All Gaussian fusion remains closed-form (information addition), no iterative solvers
- **Soft association:** No changes to association (still responsibility-based, no gating)
- **One-shot loop correction:** Loop factors remain late evidence, fused via barycenter (no changes needed for IMU work)
- **Jacobian policy:** IMU preintegration uses Jacobians (front-end sensor-to-evidence extraction, explicitly logged)
- **No ground-truth ingestion:** Backend never subscribes to GT topics (verified in tests)
- **OpReport taxonomy:** Every new operator emits OpReport with required fields

#### 2.7 Adaptive IMU Noise Model (Future Enhancement)

**Reference**: `docs/Self-Adaptive Systems Guide.md` Section 1 (Adaptive Noise Covariance via Wishart Conjugate Updates)

**Future Work:**
- Integrate `AdaptiveIMUNoiseModel` from Self-Adaptive Systems Guide
- Replace fixed IMU noise parameters with online Wishart conjugate updates
- Track accelerometer/gyroscope noise and bias random walk separately
- Use adaptive forgetting factor inferred from changepoint/hazard model
- Emit diagnostics: noise covariance traces, anomaly detection

**Files (to be created):**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/adaptive_noise.py` (Wishart prior implementation)
- Integration with `imu_jax_kernel.py` for adaptive noise in sigma-point propagation

**Design Invariants:**
- No hard gates: noise adaptation via continuous Wishart updates, not threshold-based rejection
- Startup is not a mode: behavior emerges from prior effective sample size, not time-based branching
- Constants surfaced as priors: initial confidence (ESS), forgetting factor (hazard rate Beta hyperparameters)

---

## 3) Medium-Term: Alternative Datasets & Algorithm Fixes

### A) SE(3) drift investigation

**Symptom:** Trajectory blows up (e.g., kilometers instead of meters).

**Primary files:**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/backend_node.py`
- `fl_ws/src/fl_slam_poc/fl_slam_poc/common/se3.py` (flattened from `common/transforms/se3.py`)
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/tb3_odom_bridge.py` (moved from `utility_nodes/` during flattening)

**Checklist:**
- Verify pose composition conventions and frame semantics (odom/base).
- Confirm odometry is delta (twist-integrated) where expected.
- Validate quaternion/rotvec conversions and covariance transport.

### B) Timestamp monotonicity

**Symptom:** Remaining non-monotonic gaps / duplicates impacting association and evaluation.

**Primary files:**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/sensor_io.py` (flattened from `frontend/processing/sensor_io.py`)
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/timestamp.py` (flattened from `backend/parameters/timestamp.py`)
- `tools/align_ground_truth.py`

### C) TurtleBot3 (2D) validation

**Files:**
- `phase2/fl_ws/src/fl_slam_poc/launch/poc_tb3_rosbag.launch.py`
- `tools/download_tb3_rosbag.sh`

### D) NVIDIA r2b (3D) validation / GPU

**Files:**
- `phase2/fl_ws/src/fl_slam_poc/launch/poc_3d_rosbag.launch.py`
- `tools/download_r2b_dataset.sh`
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/pointcloud_gpu.py` (flattened from `frontend/loops/pointcloud_gpu.py`)

---

## 4) Future Work: Camera-Frame Gaussian Splat Map with vMF Shading

### Overview

This section documents the full specification for a **camera-frame Gaussian splat map** (K=512 splats, J=2 vMF lobes per splat) with view-dependent appearance modeling. This approach replaces histogram-based RGB-D descriptors with a continuous, probabilistic 3D map representation suitable for dense appearance modeling and loop closure.

**Key design principle:** The splat map is a **declared generative model** with exact EM-style updates (E-step: soft responsibilities, M-step: closed-form projections). Outlier handling and spatial/photometric likelihoods are explicit model components, not heuristics.

---

### 4.1 System Mode & Assumptions

**Mode 1: Camera-Frame Map (Recommended)**

- **Map frame:** `camera_color_optical_frame` (avoids TF dependency, robust to missing extrinsics)
- **Camera center:** Origin (c_t = 0 in camera frame)
- **View direction for splat k:**
  ```
  v_k = Î¼_k / (||Î¼_k|| + Îµ)
  ```
  with clamping for near-origin splats (see Â§4.6.2)

**Inputs (synchronized):**
- RGB (decompressed): `/camera/image_raw` (sensor_msgs/Image, frame_id = camera_color_optical_frame)
- Depth (aligned, decompressed): `/camera/depth/image_raw` (sensor_msgs/Image, same frame_id)
- Odometry `/odom`: optional for logging/recording, not required for core camera-frame inference

---

### 4.2 Intrinsics & Depth Conversion

**Camera intrinsics (fallback if CameraInfo unavailable):**

```
K = [[383,   0, 320],
     [  0, 383, 240],
     [  0,   0,   1]]
```

**Depth scale parameter (mandatory):**
- Depth encoding varies by sensor:
  - `16UC1` often stores depth in **millimeters**
  - `32FC1` often stores depth in **meters**
- Define `depth_scale` such that `z = depth_scale * d` is in meters
- Default suggestions (must confirm message encoding):
  - If 16U mm: `depth_scale = 0.001`
  - If 32F m: `depth_scale = 1.0`

**Backprojection:**

For pixel (u, v) with depth d:

```
z = depth_scale * d
x_camera = z * [(u - cx)/fx, (v - cy)/fy, 1]^T
```

---

### 4.3 Noise Model & Pixel Weight (By-Construction Likelihood)

Add parameter `depth_noise_model` with:

**Linear-in-depth noise:**

```
Ïƒ_z(z) = a + b*z
w = 1 / Ïƒ_z(z)^2
```

**Interpretation:** `w` is the precision of spatial observation; this is strictly within the likelihood model (not a heuristic).

**Diagnostic:** Log mean `w` per frame. If it collapses to near-zero, likely indicates wrong depth scale or intrinsics.

---

### 4.4 Map Parameters (Per Splat)

Configuration: **K = 512 splats**, **J = 2 directional lobes per splat**

**Geometry (per splat k):**

- Position: `Î¼_k âˆˆ â„Â³`
- Covariance: `Î£_k = diag(ÏƒÂ²_{k,x}, ÏƒÂ²_{k,y}, ÏƒÂ²_{k,z})`
- Mixture weight: `Ï€_k âˆˆ (0, 1)` with `Î£_k Ï€_k = 1`

**Directional gating (vMF lobes):**

- Mean directions: `m_{k,1}, m_{k,2} âˆˆ SÂ²`
- Concentrations: `Îº_{k,1}, Îº_{k,2} â‰¥ 0`
- Gating (softmax over lobes):
  ```
  r_{k,j}(v) = softmax_j(Îº_{k,j} * m_{k,j}^T * v)
  ```

**Appearance (Gaussian in Lab color space, per lobe):**

- Color mean: `Î¼^(y)_{k,j} âˆˆ â„Â³` (Lab coordinates)
- Color covariance: `Î£^(y)_{k,j} = diag(ÏƒÂ²_{k,j,L}, ÏƒÂ²_{k,j,a}, ÏƒÂ²_{k,j,b})`

**Predicted appearance (view-dependent mixture):**

```
Î¼Ì‚^(y)_k(v) = Î£_j r_{k,j}(v) * Î¼^(y)_{k,j}
Î£Ì‚^(y)_k(v) = Î£_j r_{k,j}(v) * Î£^(y)_{k,j} + Îµ_y*I
p(y | k, v) = ð’©(y; Î¼Ì‚^(y)_k(v), Î£Ì‚^(y)_k(v))
```

---

### 4.5 Outlier Model (Photometric-Only for Camera-Frame Mode)

**Outlier parameters:**

- Outlier mixture weight: `Ï€_out`
- Outlier appearance distribution: `p_out(y) = ð’©(y; Î¼_out, Î£_out)` with large variance (or uniform-ish approximation)

**Note:** Spatial outlier term omitted in v1 camera-frame mode (will be added in Mode 2 world-map mode).

**Responsibilities (E-step):**

For pixel p with observation (x, y) and view direction v_k:

```
Î³_{p,k} = [Ï€_k * p(x|k) * p(y|k,v_k)] / [Î£_{k'âˆˆK_p} Ï€_{k'} * p(x|k') * p(y|k',v_{k'}) + Ï€_out * p_out(y)]

Î³_{p,out} = [Ï€_out * p_out(y)] / [Î£_{k'âˆˆK_p} Ï€_{k'} * p(x|k') * p(y|k',v_{k'}) + Ï€_out * p_out(y)]
```

**Diagnostic:** If mean outlier mass per frame > 0.3â€“0.5, flag "intrinsics/depth mismatch likely."

**Important:** This is a diagnostic threshold only. If we want adaptive behavior (e.g., raising `Ï€_out` or spawning dynamic splats), it must be an explicit, declared meta-operator with logged triggers (not heuristic gating).

---

### 4.6 Candidate Selection & Numeric Safety

**4.6.1 Pixel Subsampling (Performance)**

Prototype defaults (RTX 4050 friendly):

- Stride: 4â€“8 pixels
- Cap to `max_pixels = 10kâ€“20k` per frame

Only consider pixels with:

- Valid depth: `z > z_min` (e.g., 0.1m)
- Optional depth range: `z < z_max` (e.g., 10m)

**4.6.2 View Direction Safety**

Compute `v_k` with clamping:

```
v_k = Î¼_k / max(||Î¼_k||, Ï)
```

with `Ï = 0.1m` (suggested).

If `||Î¼_k|| < Ï`, either:

- Skip shading update for that splat this frame, or
- Clamp as above and proceed

This avoids NaNs from near-origin splats.

**4.6.3 KNN Candidate Selection**

For each observation point x:

- Choose `M = 16` nearest neighbors among `Î¼_k` in Euclidean distance
- At K=512, CPU KNN is sufficient (use FAISS CPU, FLANN, or brute-force)
- GPU KNN is optional but supported

---

### 4.7 Per-Frame Inference & Accumulator Updates

**4.7.1 Frame Preprocessing**

1. Decompress RGB + depth images
2. Convert RGB â†’ Lab color space (float32)
3. Subsample pixels (stride + optional random selection)
4. For each sampled pixel:

   - Compute `z`, backproject `x`
   - Compute reliability weight `w`
   - Store `(x, y, w)`

**4.7.2 Responsibilities & Sufficient-Stat Accumulation**

For each observation `(x, y, w)`:

1. **Candidate set K_p:** KNN to `Î¼_k`

2. **For each candidate k:**

   - Spatial log-likelihood:
     ```
     log p(x|k) = -Â½(x-Î¼_k)^T Î£_k^{-1} (x-Î¼_k) - Â½log|Î£_k| + C
     ```

   - View direction: `v_k` (camera-frame)
   - Gating: `r_{k,j}(v_k)`
   - Predicted color mean/cov: `Î¼Ì‚^(y)_k(v_k)`, `Î£Ì‚^(y)_k(v_k)`
   - Color log-likelihood: `log p(y|k,v_k)`
   - Combined:
     ```
     â„“_k = log Ï€_k + log p(x|k) + log p(y|k,v_k)
     ```


3. **Outlier log-likelihood:**
   ```
   â„“_out = log Ï€_out + log p_out(y)
   ```

4. **Normalize via softmax** over `{â„“_k} âˆª {â„“_out}` to get `Î³_k` and `Î³_out`

5. **Accumulate geometry:**

   - `W_k += Î³_k * w`
   - `M_k += Î³_k * w * x`
   - `Q_k += Î³_k * w * x*x^T`

6. **Accumulate vMF (per lobe j):**

   - `Î³_{k,j} = Î³_k * r_{k,j}(v_k)`
   - `W^(v)_{k,j} += Î³_{k,j} * w`
   - `S_{k,j} += Î³_{k,j} * w * v_k`

7. **Accumulate appearance (per lobe j):**

   - `W^(y)_{k,j} += Î³_{k,j} * w`
   - `M^(y)_{k,j} += Î³_{k,j} * w * y`
   - `Q^(y)_{k,j} += Î³_{k,j} * w * y*y^T`

Also accumulate frame-level diagnostics:

- Mean outlier mass
- Mean residual

**4.7.3 Apply Updates Once Per Frame (M-Step Projection)**

For each splat k with `W_k > Ï„`:

**Geometry:**

```
Î¼_k â† M_k / W_k
Î£_k â† diag(Q_k/W_k - Î¼_k*Î¼_k^T) + Îµ_x*I
```

**Directional lobes (per lobe j with W^(v)_{k,j} > Ï„_v):**

```
m_{k,j} â† S_{k,j} / ||S_{k,j}||
RÌ„_{k,j} = ||S_{k,j}|| / W^(v)_{k,j}
Îº_{k,j} â† clamp([RÌ„_{k,j}(3 - RÌ„_{k,j}Â²)] / [1 - RÌ„_{k,j}Â²], Îº_min, Îº_max)
```

**Appearance (per lobe j):**

```
Î¼^(y)_{k,j} â† M^(y)_{k,j} / W^(y)_{k,j}
Î£^(y)_{k,j} â† diag(Q^(y)_{k,j}/W^(y)_{k,j} - Î¼^(y)_{k,j}*Î¼^(y)_{k,j}^T) + Îµ_y*I
```

Reset accumulators for next frame (or use EMA if desired, but EMA must be documented as a prior/forgetting process).

---

### 4.8 Initialization (First Frame)

**4.8.1 Sampling for Diversity**

On first frame:

- Subsample with stride + random selection among valid depth pixels (avoids grid bias)
- Choose K points and set `Î¼_k = x`

**4.8.2 Covariance Initialization**

Depth noise dominates z; initialize:

- `Ïƒ_z` larger than `Ïƒ_x, Ïƒ_y` (based on noise model at that z)

**4.8.3 Mixture Mass (Ï€_k)**

Initialize uniform, optionally boost nearer points:

```
Ï€_k âˆ 1 / max(z_k, Îµ)
```

then normalize over k.

**4.8.4 Lobe Initialization (Avoid Perfect Opposition)**

Set:

- `m_{k,1} = Î¼_k / ||Î¼_k||`
- `m_{k,2} = normalize(m_{k,1} + Î´)`, where `Î´` is a small random vector orthogonal-ish to `m_{k,1}`

This avoids forcing a two-lobe symmetry the data may not support.

Set initial `Îº` small (0.1â€“1.0).

**4.8.5 Appearance Initialization**

Use the pixel Lab color at the seed pixel:

- Set both lobes' means close to that color, with moderate variance

---

### 4.9 Diagnostics (Expanded)

Add the following diagnostics to validate splat map quality:

- **Observed vs rendered (predicted) image** at sampled pixels
- **Residual heatmap:** `|y - Î¼Ì‚^(y)|`
- **Mean and histogram of w** (reliability weights)
- **Active splat count:** number with `Ï€_k` or `W_k` above threshold
- **Mean outlier mass** per frame
- **Kappa histogram** (directional concentration distribution)

**Alert conditions:**

- Outlier mass > 0.3â€“0.5 persistently â‡’ intrinsics/depth_scale likely wrong
- Mean `w` unexpectedly tiny â‡’ depth scale wrong or noise model too pessimistic
- Many NaNs â‡’ view-direction clamp missing or invalid depth not filtered

---

### 4.10 World-Map Mode (Mode 2) - Future Extension

When enabling world/base map in future:

- Add spatial outlier term `p_out(x)` broad and centered on current camera center
- Add diagnostic: compare distribution of backprojected points in map frame across time; if it "breathes" incoherently, extrinsics are wrong

---

### 4.11 Implementation Files & Structure

**New files:**

- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/splat_map/splat_map.py` - Core splat map class
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/splat_map/vmf_lobe.py` - vMF directional lobe operations
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/splat_map/appearance_model.py` - Lab color Gaussian model
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/splat_map/splat_visualizer.py` - Diagnostics + rendering

**Integration points:**

- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/frontend_node.py` - Add splat map instance, update loop
- `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/sensor_io.py` - Add Lab color conversion utilities

**Message definitions:**

- New: `fl_ws/src/fl_slam_poc/msg/SplatMapState.msg` - For publishing splat map state to backend
- New: `fl_ws/src/fl_slam_poc/msg/SplatDiagnostics.msg` - For diagnostics visualization

---

### 4.12 Immediate Configuration (M3DGR Dataset)

Starting parameters for M3DGR rosbag:

```python
# Camera intrinsics (fallback if no CameraInfo)
camera_fx = 383.0
camera_fy = 383.0
camera_cx = 320.0
camera_cy = 240.0

# Depth configuration (MUST VERIFY encoding)
depth_scale = 0.001  # if 16UC1 in mm, else 1.0 if 32FC1 in m

# Splat map configuration
splat_K = 512          # number of splats
splat_J = 2            # directional lobes per splat
splat_stride = 6       # pixel sampling stride
splat_max_pixels = 15000
splat_candidate_M = 16 # KNN candidates per observation

# Noise model (linear in depth)
depth_noise_a = 0.01   # baseline noise (m)
depth_noise_b = 0.02   # depth-proportional noise (unitless)

# Depth range
depth_min = 0.1        # meters
depth_max = 10.0       # meters

# Regularization
epsilon_x = 1e-6       # spatial covariance floor
epsilon_y = 1e-4       # appearance covariance floor
kappa_min = 0.1        # minimum concentration
kappa_max = 100.0      # maximum concentration

# Outlier model
pi_outlier = 0.1       # outlier mixture weight
```

---

### 4.13 Design Invariants Compliance

**Compliance with Frobenius-Legendre invariants:**

1. **Closed-form-first:** EM updates (E-step: softmax responsibilities, M-step: weighted mean/covariance) are closed-form, exact within the declared Gaussian mixture model
2. **Soft association:** Responsibilities `Î³_{p,k}` are derived from declared likelihood model, no hard gating
3. **Declared approximations:**

   - KNN candidate selection (BudgetedRecomposition): explicit approximation operator with declared objective (spatial proximity)
   - Emit `OpReport` with `approximation_triggers: {"BudgetedRecomposition"}`, `frobenius_applied: True` (if covariance updates involve linearization)

4. **Outlier model:** Explicit mixture component, not a heuristic
5. **Operator taxonomy:** All updates emit `OpReport` with family declarations (`family_in: "GaussianMixture"`, `family_out: "GaussianMixture"`, `closed_form: True`)

---

### 4.14 Testing & Validation

**Tests:**

- `fl_ws/src/fl_slam_poc/test/test_splat_map.py` - Unit tests for splat map operations
  - Synthetic RGB-D frame â†’ verify responsibilities sum to 1
  - Verify vMF Îº estimation matches expected concentration for synthetic data
  - Verify Lab color Gaussian updates are exact weighted means

**Validation criteria:**

- Mean outlier mass < 0.2 on M3DGR sequences (indicates good intrinsics/depth_scale)
- Active splat count â‰ˆ 400-500 after 10 frames (indicates diversity maintained)
- Residual heatmap shows spatially coherent errors (not salt-and-pepper noise)
- Rendered image qualitatively matches observed image at sampled pixels

---

## 5) Long-Term Future Work

### A) Visual Loop Factors (Beyond Dense Map)

**When implemented:**

- Extract keypoint descriptors (ORB/SIFT/SuperPoint) from RGB images
- Produce bearing factors (vMF on viewing directions) for sparse features
- Emit `OpReport` for feature extraction:
  - `approximation_triggers: {"FeatureDetection"}` if detector is not rotation-invariant
  - `family_in: "Image"`, `family_out: "vMF"`, `closed_form: False` (feature extraction is not closed-form)
- Update soft association to include visual descriptor likelihood (e.g., Hamming distance â†’ Dirichlet responsibilities)

**Integration:**

- Keypoints detected in `fl_ws/src/fl_slam_poc/fl_slam_poc/frontend/loop_processor.py` (or new `keypoint_detector.py`)
- Bearing factors published to `/sim/bearing_factor` (new message type)
- Backend fuses bearing factors as vMF evidence on anchor directions

**Self-Adaptive Systems Integration**: See `docs/Self-Adaptive Systems Guide.md` Section 4 (Adaptive Loop Closure Confidence) for adaptive loop gating:
- Hellinger-based soft gating (no hard Ï‡Â² thresholds)
- Adaptive Î» threshold based on false positive rate tracking
- Trajectory consistency checking
- Certificate-based quality metrics for downstream scaling (no accept/reject branching)

---

### B) Additional Sensors

**GNSS (if available in dataset):**

- Add as 3DOF position factor with horizontal/vertical uncertainty
- Construct information form with anisotropic covariance:
  - `sigma_horizontal = 2-5m` (typical GPS)
  - `sigma_vertical = 5-10m` (GPS vertical is weak)
- Emit `OpReport`: `family_in: "Gaussian"`, `family_out: "Gaussian"`, `closed_form: True` (exact information fusion)

**Semantic observations (if available):**

- Add as Dirichlet factors on object categories
- Use soft responsibilities over semantic classes (no argmax)
- Emit `OpReport`: `family_in: "Categorical"`, `family_out: "Dirichlet"`, `closed_form: True` (conjugate update)

---

### C) Backend Optimizations

**Hierarchical recomposition for large loop scopes:**

- Use associativity of Bregman barycenters (exact, not an approximation)
- Recursively fuse loop evidence in clusters before global recomposition
- Emit `OpReport`: `approximation_triggers: {}` (exact), `closed_form: True`

**Budgeted recomposition for real-time constraints:**

- Declared approximation operator: `BudgetedRecomposition(e_loop, budget B)`
- Objective: maximize Bregman divergence reduction or mutual information (closed-form for Gaussian/Dirichlet)
- Emit `OpReport`:
  - `approximation_triggers: {"BudgetedRecomposition"}`
  - `frobenius_applied: True` (third-order correction to transport error)
  - Log: loop factor id, selected scope, objective value, diagnostics (predicted vs realized posterior change)

**Self-Adaptive Systems Integration**: See `docs/Self-Adaptive Systems Guide.md` Section 5 (Adaptive Frobenius Correction Strength):
- Adaptive Î² correction strength based on observed linearization error
- Certificate-based quality metrics (correction magnitude, expected error reduction)
- No hard gates: continuous scaling based on certificate quality

---

### D) Dirichlet Semantic SLAM Integration

**Files:**
- `phase2/fl_ws/src/fl_slam_poc/fl_slam_poc/nodes/dirichlet_backend_node.py`
- `phase2/fl_ws/src/fl_slam_poc/fl_slam_poc/nodes/sim_semantics_node.py`
- `fl_ws/src/fl_slam_poc/fl_slam_poc/common/dirichlet_geom.py` (moved from `operators/` during flattening)

**Self-Adaptive Systems Integration**: See `docs/Self-Adaptive Systems Guide.md` Section 3 (Adaptive Association via Dirichlet Concentration Tracking):
- Adaptive concentration regulation based on system-wide entropy
- Entropy-based confidence tracking (no hard gating)
- Hellinger-based entity similarity for robust association
- Temporal decay for dynamic scenes

---

### E) Gazebo Live Testing

**Files:**
- `phase2/fl_ws/src/fl_slam_poc/launch/poc_tb3.launch.py`
- `phase2/fl_ws/src/fl_slam_poc/fl_slam_poc/utility_nodes/sim_world.py`

---

### F) Visualization

**RViz:** `config/fl_slam_rviz.rviz` (local/optional)
**Rerun bridge:** removed from MVP; revisit later if needed

### G) Self-Adaptive System Integration

**Reference**: `docs/Self-Adaptive Systems Guide.md`

**System-Wide Adaptive Coordinator** (Section 6):
- Health monitoring across all subsystems (IMU, odom, loops, association)
- Cross-system adaptation via expected-utility maximization (myopic, single-step)
- Adaptation budget: maximize benefit (divergence reduction) subject to frame budget
- Graceful degradation: when one subsystem degrades, others compensate via continuous scaling

**Monge-AmpÃ¨re Transport for Dynamic Maps** (Section 7):
- Optimal transport for dynamic scene handling (moving objects, scene changes)
- Hellinger trigger for transport initiation
- Scene flow adapter for filtering dynamic points in SLAM
- Equivariant Wishart transport for covariance adaptation

**Integration Points:**
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/adaptive_coordinator.py` (to be created)
- `fl_ws/src/fl_slam_poc/fl_slam_poc/backend/adaptive_transport.py` (to be created)
- Diagnostics publisher: `/cdwm/adaptive_diagnostics` topic

**Design Invariants Compliance:**
- No hard gates: all adaptations via continuous scaling, not accept/reject branching
- Startup is not a mode: behavior emerges from priors, not time-based logic
- Expected vs realized benefit: internal objectives only (divergence reduction, ELBO increase), not external metrics (ATE/RPE)
- Constants surfaced as priors: effective sample size, hazard rates, certificate risk levels, frame budgets

---

## Roadmap Summary

### Immediate Work (Priority 1)
1. âœ… IMU integration + 15D state (Phases 1-2) - **COMPLETED**
2. Wheel odom separation + factor scheduling (Phase 3)
3. Dense RGB-D in 3D mode (Phase 4)
4. Provenance + evaluation hardening (Phases 5-6)
5. Self-adaptive systems integration (see Section 5.G)

### Near-Term Future Work (Priority 2)
- Camera-frame Gaussian splat map with vMF shading (K=512, J=2)
- Replaces histogram-based RGB-D descriptors
- Enables view-dependent appearance modeling for loop closure

### Medium-Term (Priority 3)
- Alternative datasets (TurtleBot3, NVIDIA r2b)
- GPU acceleration
- Gazebo live testing

### Long-Term (Priority 4)
- Visual loop factors (bearing-based) with adaptive gating
- GNSS integration (if dataset available) with adaptive sensor weighting
- Semantic observations (Dirichlet factors) with adaptive concentration
- Backend optimizations (hierarchical/budgeted recomposition) with adaptive Frobenius correction
- Dirichlet semantic SLAM integration with adaptive association
- Monge-AmpÃ¨re transport for dynamic maps
- System-wide adaptive coordinator
- Visualization enhancements

---

**End of Roadmap**
