# FL-SLAM Self-Adaptive Systems Guide
## Leveraging Information Geometry for Robust Edge-Case Handling

---

## Philosophy: Why Self-Adaptation?

Traditional SLAM systems rely on hand-tuned parameters that work well on average but fail catastrophically in edge cases:
- IMU noise spikes during high-dynamics maneuvers
- Sensor degradation over time (dust on cameras, magnetometer drift)
- Environmental changes (lighting, texture-poor regions)
- Unexpected motion patterns (collisions, slippage)

**Information geometry provides principled self-adaptation** because:
1. The Fisher metric naturally measures "how much information" each sensor provides
2. Conjugate priors enable online Bayesian updating of noise models
3. Divergence measures detect when models no longer fit observations
4. Exponential family structure ensures updates remain tractable

---

## 1. Adaptive Noise Covariance via Wishart Conjugate Updates

### 1.1 Mathematical Foundation

The Wishart distribution $W_p(n, \Sigma)$ is the conjugate prior for the precision matrix (inverse covariance) of a multivariate Gaussian.

**Key Properties:**
- **Additivity:** $W_1 + W_2 \sim W_p(n_1 + n_2, \Sigma)$ under independent summation
- **Equivariance:** $AWA^T \sim W_q(n, A\Sigma A^T)$ under linear transformation
- **Mean:** $\mathbb{E}[W] = n\Sigma$

**Conjugate Update Rule:**

Given prior $\Lambda \sim W_p(n_0, S_0)$ and observations $\{x_i\}_{i=1}^N$ from $\mathcal{N}(\mu, \Lambda^{-1})$:

$$\Lambda | \{x_i\} \sim W_p\left(n_0 + N, \left(S_0^{-1} + \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^T\right)^{-1}\right)$$

The posterior mean precision is:
$$\hat{\Lambda} = (n_0 + N) \left(S_0^{-1} + \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^T\right)^{-1}$$

### 1.2 Application: Adaptive IMU Noise Model

```python
# New file: backend/adaptive_noise.py

import jax.numpy as jnp
from jax.scipy.linalg import inv, cholesky
from dataclasses import dataclass
from typing import Tuple

@dataclass
class WishartPrior:
    """Wishart prior for precision matrix."""
    n: float           # Degrees of freedom (confidence)
    S: jnp.ndarray     # Scale matrix (n × S = expected precision)
    
    @property
    def dim(self) -> int:
        return self.S.shape[0]
    
    @property
    def mean_precision(self) -> jnp.ndarray:
        """E[Λ] = n * S"""
        return self.n * self.S
    
    @property
    def mean_covariance(self) -> jnp.ndarray:
        """E[Σ] ≈ S⁻¹ / (n - p - 1) for n > p + 1"""
        return inv(self.S) / (self.n - self.dim - 1)
    
    def update(self, residuals: jnp.ndarray) -> 'WishartPrior':
        """
        Bayesian update given new residuals.
        
        Args:
            residuals: (N, dim) array of observation residuals
            
        Returns:
            Updated WishartPrior
        """
        N = residuals.shape[0]
        scatter = residuals.T @ residuals  # Σᵢ rᵢ rᵢᵀ
        
        # Conjugate update
        S_inv_new = inv(self.S) + scatter
        S_new = inv(S_inv_new)
        n_new = self.n + N
        
        return WishartPrior(n=n_new, S=S_new)
    
    def update_with_forgetting(
        self, 
        residuals: jnp.ndarray, 
        forgetting_factor: float = 0.99
    ) -> 'WishartPrior':
        """
        Update with exponential forgetting for non-stationary noise.
        
        The forgetting factor γ ∈ (0,1] downweights old observations:
        - γ = 1.0: standard Bayesian update (stationary assumption)
        - γ = 0.95: effective window ~20 observations
        - γ = 0.99: effective window ~100 observations
        """
        N = residuals.shape[0]
        scatter = residuals.T @ residuals
        
        # Apply forgetting to prior
        n_forgotten = forgetting_factor * self.n
        S_inv_forgotten = forgetting_factor * inv(self.S)
        
        # Then update
        S_inv_new = S_inv_forgotten + scatter
        S_new = inv(S_inv_new)
        n_new = n_forgotten + N
        
        return WishartPrior(n=n_new, S=S_new)


class AdaptiveIMUNoiseModel:
    """
    Self-adaptive IMU noise model using Wishart conjugate updates.
    
    Maintains separate priors for:
    - Accelerometer noise covariance (3x3)
    - Gyroscope noise covariance (3x3)
    - Accelerometer bias random walk (3x3)
    - Gyroscope bias random walk (3x3)
    """
    
    def __init__(
        self,
        accel_noise_prior: WishartPrior,
        gyro_noise_prior: WishartPrior,
        accel_bias_prior: WishartPrior,
        gyro_bias_prior: WishartPrior,
        forgetting_factor: float = 0.995
    ):
        self.accel_noise = accel_noise_prior
        self.gyro_noise = gyro_noise_prior
        self.accel_bias = accel_bias_prior
        self.gyro_bias = gyro_bias_prior
        self.gamma = forgetting_factor
        
        # Track adaptation history for diagnostics
        self.history = {
            'accel_noise_trace': [],
            'gyro_noise_trace': [],
            'accel_bias_trace': [],
            'gyro_bias_trace': []
        }
    
    def update_from_preintegration_residuals(
        self,
        accel_residuals: jnp.ndarray,  # (N, 3) accelerometer prediction errors
        gyro_residuals: jnp.ndarray,   # (N, 3) gyroscope prediction errors
    ):
        """Update noise models from preintegration residuals."""
        self.accel_noise = self.accel_noise.update_with_forgetting(
            accel_residuals, self.gamma
        )
        self.gyro_noise = self.gyro_noise.update_with_forgetting(
            gyro_residuals, self.gamma
        )
        
        # Log for diagnostics
        self.history['accel_noise_trace'].append(
            jnp.trace(self.accel_noise.mean_covariance)
        )
        self.history['gyro_noise_trace'].append(
            jnp.trace(self.gyro_noise.mean_covariance)
        )
    
    def update_from_bias_innovations(
        self,
        accel_bias_innovations: jnp.ndarray,  # (N, 3) bias estimate changes
        gyro_bias_innovations: jnp.ndarray,   # (N, 3) bias estimate changes
        dt: float
    ):
        """Update bias random walk models from bias estimation innovations."""
        # Scale innovations by 1/√dt to get random walk intensity
        accel_rw = accel_bias_innovations / jnp.sqrt(dt)
        gyro_rw = gyro_bias_innovations / jnp.sqrt(dt)
        
        self.accel_bias = self.accel_bias.update_with_forgetting(accel_rw, self.gamma)
        self.gyro_bias = self.gyro_bias.update_with_forgetting(gyro_rw, self.gamma)
        
        self.history['accel_bias_trace'].append(
            jnp.trace(self.accel_bias.mean_covariance)
        )
        self.history['gyro_bias_trace'].append(
            jnp.trace(self.gyro_bias.mean_covariance)
        )
    
    def get_current_noise_params(self) -> dict:
        """Get current MAP estimates of noise parameters."""
        return {
            'accel_noise_cov': self.accel_noise.mean_covariance,
            'gyro_noise_cov': self.gyro_noise.mean_covariance,
            'accel_bias_cov': self.accel_bias.mean_covariance,
            'gyro_bias_cov': self.gyro_bias.mean_covariance,
            # Confidence (higher = more certain)
            'accel_noise_confidence': self.accel_noise.n,
            'gyro_noise_confidence': self.gyro_noise.n,
        }
    
    def detect_anomaly(self, threshold_factor: float = 3.0) -> dict:
        """
        Detect if recent noise levels are anomalous compared to history.
        
        Returns dict with anomaly flags and severity scores.
        """
        def check_anomaly(trace_history, name):
            if len(trace_history) < 10:
                return {'anomaly': False, 'severity': 0.0}
            
            recent = jnp.mean(jnp.array(trace_history[-5:]))
            baseline = jnp.mean(jnp.array(trace_history[-50:-5])) if len(trace_history) > 50 else jnp.mean(jnp.array(trace_history[:-5]))
            baseline_std = jnp.std(jnp.array(trace_history[:-5]))
            
            if baseline_std < 1e-10:
                return {'anomaly': False, 'severity': 0.0}
            
            z_score = (recent - baseline) / baseline_std
            return {
                'anomaly': z_score > threshold_factor,
                'severity': float(z_score),
                'recent': float(recent),
                'baseline': float(baseline)
            }
        
        return {
            'accel_noise': check_anomaly(self.history['accel_noise_trace'], 'accel'),
            'gyro_noise': check_anomaly(self.history['gyro_noise_trace'], 'gyro'),
            'accel_bias': check_anomaly(self.history['accel_bias_trace'], 'accel_bias'),
            'gyro_bias': check_anomaly(self.history['gyro_bias_trace'], 'gyro_bias'),
        }


def create_default_imu_adaptive_model(
    accel_noise_std: float = 0.1,      # m/s² initial estimate
    gyro_noise_std: float = 0.01,      # rad/s initial estimate  
    accel_bias_std: float = 0.001,     # m/s²/√s initial estimate
    gyro_bias_std: float = 0.0001,     # rad/s/√s initial estimate
    initial_confidence: float = 10.0,  # Degrees of freedom (low = uncertain)
    forgetting_factor: float = 0.995
) -> AdaptiveIMUNoiseModel:
    """Create adaptive IMU model with reasonable defaults."""
    
    def make_prior(std, dim=3):
        # S chosen so that n*S gives initial precision estimate
        cov = std**2 * jnp.eye(dim)
        prec = inv(cov)
        S = prec / initial_confidence
        return WishartPrior(n=initial_confidence, S=S)
    
    return AdaptiveIMUNoiseModel(
        accel_noise_prior=make_prior(accel_noise_std),
        gyro_noise_prior=make_prior(gyro_noise_std),
        accel_bias_prior=make_prior(accel_bias_std),
        gyro_bias_prior=make_prior(gyro_bias_std),
        forgetting_factor=forgetting_factor
    )
```

### 1.3 Integration with `imu_jax_kernel.py`

```python
# Modifications to imu_jax_kernel.py

class IMUPreintegrator:
    def __init__(self, adaptive_noise: AdaptiveIMUNoiseModel = None):
        self.adaptive_noise = adaptive_noise or create_default_imu_adaptive_model()
    
    def preintegrate(self, imu_measurements: jnp.ndarray, dt: float) -> PreintegratedIMU:
        # Get current noise estimates
        noise_params = self.adaptive_noise.get_current_noise_params()
        
        # Use adaptive covariances in preintegration
        Q_accel = noise_params['accel_noise_cov']
        Q_gyro = noise_params['gyro_noise_cov']
        
        # ... standard preintegration with adaptive Q ...
        
        return result
    
    def update_noise_model(self, predicted: State, measured: State):
        """Call after each optimization to update noise model."""
        # Compute residuals between predicted and optimized state
        accel_residuals = extract_accel_residuals(predicted, measured)
        gyro_residuals = extract_gyro_residuals(predicted, measured)
        
        self.adaptive_noise.update_from_preintegration_residuals(
            accel_residuals, gyro_residuals
        )
        
        # Check for anomalies
        anomalies = self.adaptive_noise.detect_anomaly()
        if any(a['anomaly'] for a in anomalies.values()):
            self.handle_noise_anomaly(anomalies)
```

---

## 2. Adaptive Sensor Weighting via Fisher Information

### 2.1 Mathematical Foundation

The Fisher Information quantifies how much "statistical information" a sensor provides about the state:

$$I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(x|\theta)}{\partial \theta}\right)^2\right] = -\mathbb{E}\left[\frac{\partial^2 \log p(x|\theta)}{\partial \theta^2}\right]$$

**Key Insight:** Sensors with higher Fisher Information should receive higher weight in fusion.

For Gaussian observations with covariance $\Sigma$:
$$I = \Sigma^{-1}$$

So adaptive covariance directly translates to adaptive weighting!

### 2.2 Application: Dynamic Sensor Weighting

```python
# New file: backend/adaptive_fusion.py

import jax.numpy as jnp
from typing import List, Tuple
from dataclasses import dataclass

@dataclass
class SensorObservation:
    """Observation from a sensor with uncertainty."""
    value: jnp.ndarray           # Observation in natural parameters
    information: jnp.ndarray     # Fisher information (precision)
    sensor_id: str
    timestamp: float

@dataclass
class AdaptiveSensorModel:
    """Tracks sensor reliability over time."""
    sensor_id: str
    
    # Wishart prior for observation noise
    noise_prior: WishartPrior
    
    # Performance tracking
    recent_residuals: List[jnp.ndarray]
    residual_window: int = 50
    
    # Reliability score (0 = unreliable, 1 = fully reliable)
    reliability: float = 1.0
    
    # Hellinger divergence from expected behavior
    divergence_history: List[float] = None
    
    def __post_init__(self):
        if self.divergence_history is None:
            self.divergence_history = []


class AdaptiveFusionEngine:
    """
    Fuses multiple sensors with adaptive weighting based on:
    1. Online-estimated noise covariances (Wishart)
    2. Recent residual performance (reliability scoring)
    3. Hellinger divergence from expected behavior (anomaly detection)
    """
    
    def __init__(self, forgetting_factor: float = 0.99):
        self.sensors: dict[str, AdaptiveSensorModel] = {}
        self.gamma = forgetting_factor
    
    def register_sensor(
        self, 
        sensor_id: str, 
        initial_noise_cov: jnp.ndarray,
        initial_confidence: float = 10.0
    ):
        """Register a new sensor with initial noise estimate."""
        dim = initial_noise_cov.shape[0]
        prec = jnp.linalg.inv(initial_noise_cov)
        S = prec / initial_confidence
        
        self.sensors[sensor_id] = AdaptiveSensorModel(
            sensor_id=sensor_id,
            noise_prior=WishartPrior(n=initial_confidence, S=S),
            recent_residuals=[],
        )
    
    def update_sensor_model(
        self, 
        sensor_id: str, 
        residual: jnp.ndarray,
        expected_residual_cov: jnp.ndarray
    ):
        """Update sensor model after observing a residual."""
        model = self.sensors[sensor_id]
        
        # Update Wishart prior
        model.noise_prior = model.noise_prior.update_with_forgetting(
            residual.reshape(1, -1), self.gamma
        )
        
        # Track residual for reliability scoring
        model.recent_residuals.append(residual)
        if len(model.recent_residuals) > model.residual_window:
            model.recent_residuals.pop(0)
        
        # Compute Hellinger divergence from expected
        observed_cov = self._estimate_residual_cov(model.recent_residuals)
        h2 = self._gaussian_hellinger_squared(observed_cov, expected_residual_cov)
        model.divergence_history.append(float(h2))
        
        # Update reliability score
        model.reliability = self._compute_reliability(model)
    
    def _estimate_residual_cov(self, residuals: List[jnp.ndarray]) -> jnp.ndarray:
        """Estimate covariance from recent residuals."""
        if len(residuals) < 2:
            return jnp.eye(residuals[0].shape[0]) if residuals else jnp.eye(3)
        
        r = jnp.stack(residuals)
        return jnp.cov(r.T) + 1e-6 * jnp.eye(r.shape[1])  # Regularize
    
    def _gaussian_hellinger_squared(
        self, 
        cov1: jnp.ndarray, 
        cov2: jnp.ndarray
    ) -> float:
        """Squared Hellinger distance between zero-mean Gaussians."""
        # H² = 1 - det(Σ₁)^(1/4) det(Σ₂)^(1/4) / det((Σ₁+Σ₂)/2)^(1/2)
        det1 = jnp.linalg.det(cov1)
        det2 = jnp.linalg.det(cov2)
        det_avg = jnp.linalg.det((cov1 + cov2) / 2)
        
        bc = (det1 ** 0.25) * (det2 ** 0.25) / (det_avg ** 0.5)
        return 1.0 - bc
    
    def _compute_reliability(self, model: AdaptiveSensorModel) -> float:
        """
        Compute reliability score based on:
        1. Consistency of noise estimates (low variance = reliable)
        2. Hellinger divergence from expected (low = reliable)
        3. Residual magnitude (low = reliable)
        """
        if len(model.divergence_history) < 5:
            return 1.0  # Insufficient data, assume reliable
        
        # Hellinger-based reliability: exp(-λ * H²)
        recent_h2 = jnp.mean(jnp.array(model.divergence_history[-10:]))
        hellinger_reliability = jnp.exp(-5.0 * recent_h2)
        
        # Residual magnitude reliability
        if model.recent_residuals:
            recent_norms = [jnp.linalg.norm(r) for r in model.recent_residuals[-10:]]
            avg_norm = jnp.mean(jnp.array(recent_norms))
            # Soft threshold based on expected magnitude
            expected_norm = jnp.sqrt(jnp.trace(model.noise_prior.mean_covariance))
            norm_ratio = avg_norm / (expected_norm + 1e-6)
            magnitude_reliability = jnp.exp(-0.5 * (norm_ratio - 1.0)**2)
        else:
            magnitude_reliability = 1.0
        
        # Combine (geometric mean)
        return float(jnp.sqrt(hellinger_reliability * magnitude_reliability))
    
    def get_adaptive_weights(self, sensor_ids: List[str]) -> jnp.ndarray:
        """
        Get fusion weights for sensors, incorporating:
        - Fisher information (precision)
        - Reliability scores
        """
        weights = []
        for sid in sensor_ids:
            model = self.sensors[sid]
            
            # Base weight from Fisher information (trace of precision)
            fisher_weight = jnp.trace(model.noise_prior.mean_precision)
            
            # Scale by reliability
            adaptive_weight = fisher_weight * model.reliability
            
            weights.append(adaptive_weight)
        
        # Normalize
        weights = jnp.array(weights)
        return weights / jnp.sum(weights)
    
    def fuse_observations(
        self, 
        observations: List[SensorObservation],
        prior: GaussianInfo = None
    ) -> GaussianInfo:
        """
        Fuse observations with adaptive weighting.
        
        Uses Bregman barycenter with weights determined by:
        - Sensor information (precision)
        - Sensor reliability (tracked performance)
        """
        sensor_ids = [obs.sensor_id for obs in observations]
        weights = self.get_adaptive_weights(sensor_ids)
        
        # Collect natural parameters
        etas = []
        for obs in observations:
            model = self.sensors[obs.sensor_id]
            # Use adaptive precision, not the observation's claimed precision
            adaptive_precision = model.noise_prior.mean_precision * model.reliability
            eta = adaptive_precision @ obs.value  # Information vector
            etas.append(eta)
        
        if prior is not None:
            etas.append(prior.to_expectation_params())
            weights = jnp.concatenate([weights, jnp.array([1.0])])
            weights = weights / jnp.sum(weights)
        
        # Bregman barycenter in expectation space
        eta_fused = jnp.sum(weights[:, None] * jnp.stack(etas), axis=0)
        
        return GaussianInfo.from_expectation_params(eta_fused)
    
    def get_diagnostics(self) -> dict:
        """Get diagnostic information about all sensors."""
        return {
            sid: {
                'reliability': model.reliability,
                'noise_cov_trace': float(jnp.trace(model.noise_prior.mean_covariance)),
                'confidence': model.noise_prior.n,
                'recent_divergence': model.divergence_history[-1] if model.divergence_history else 0.0
            }
            for sid, model in self.sensors.items()
        }
```

---

## 3. Adaptive Association via Dirichlet Concentration Tracking

### 3.1 Mathematical Foundation

The Dirichlet distribution concentration parameter $\alpha_0 = \sum_k \alpha_k$ controls how "peaked" the distribution is:
- Low $\alpha_0$: Sparse, peaked distributions (confident associations)
- High $\alpha_0$: Diffuse distributions (uncertain associations)

**Self-Adaptation:** Track the "effective concentration" of association posteriors over time. If associations become too diffuse, the system is losing track; if too peaked, it may be over-confident.

### 3.2 Application: Adaptive Soft Association

```python
# Enhancements to dirichlet_routing.py

import jax.numpy as jnp
from typing import List, Tuple, Optional
from dataclasses import dataclass, field

@dataclass
class EntityTrack:
    """Tracked entity with Dirichlet association model."""
    entity_id: int
    pseudo_counts: jnp.ndarray  # Dirichlet α parameters
    last_seen: float            # Timestamp
    
    # Adaptation parameters
    concentration_history: List[float] = field(default_factory=list)
    association_entropy_history: List[float] = field(default_factory=list)
    
    @property
    def concentration(self) -> float:
        """Total concentration α₀ = Σαₖ"""
        return float(jnp.sum(self.pseudo_counts))
    
    @property
    def entropy(self) -> float:
        """Entropy of the Dirichlet mean (categorical)."""
        p = self.pseudo_counts / jnp.sum(self.pseudo_counts)
        return -float(jnp.sum(p * jnp.log(p + 1e-10)))


class AdaptiveDirichletRouter:
    """
    Self-adaptive soft association routing with:
    1. Automatic concentration regulation
    2. Entropy-based confidence tracking
    3. Hellinger-based entity similarity
    4. Temporal decay for dynamic scenes
    """
    
    def __init__(
        self,
        num_categories: int,
        base_concentration: float = 1.0,
        min_concentration: float = 0.1,
        max_concentration: float = 100.0,
        temporal_decay: float = 0.99,
        target_entropy_fraction: float = 0.5  # Target entropy as fraction of max
    ):
        self.num_categories = num_categories
        self.base_concentration = base_concentration
        self.min_concentration = min_concentration
        self.max_concentration = max_concentration
        self.temporal_decay = temporal_decay
        self.target_entropy = target_entropy_fraction * jnp.log(num_categories)
        
        self.entities: dict[int, EntityTrack] = {}
        self.next_entity_id = 0
        
        # Global adaptation state
        self.global_entropy_history: List[float] = []
        self.concentration_scale: float = 1.0  # Adaptive multiplier
    
    def create_entity(self, initial_category: Optional[int] = None) -> int:
        """Create a new entity track."""
        entity_id = self.next_entity_id
        self.next_entity_id += 1
        
        if initial_category is not None:
            # Peaked at initial category
            alpha = jnp.ones(self.num_categories) * self.base_concentration * 0.1
            alpha = alpha.at[initial_category].set(self.base_concentration * 5.0)
        else:
            # Uniform prior
            alpha = jnp.ones(self.num_categories) * self.base_concentration
        
        self.entities[entity_id] = EntityTrack(
            entity_id=entity_id,
            pseudo_counts=alpha,
            last_seen=0.0
        )
        
        return entity_id
    
    def compute_responsibilities(
        self,
        observation_likelihoods: jnp.ndarray,  # (num_entities, num_categories)
        entity_ids: List[int],
        new_entity_prior: float = 0.1
    ) -> Tuple[jnp.ndarray, float]:
        """
        Compute soft association responsibilities.
        
        Returns:
            responsibilities: (num_entities,) array
            new_entity_responsibility: scalar
        """
        # Get current Dirichlet means as category distributions
        entity_probs = []
        for eid in entity_ids:
            alpha = self.entities[eid].pseudo_counts
            p = alpha / jnp.sum(alpha)
            entity_probs.append(p)
        
        entity_probs = jnp.stack(entity_probs)  # (num_entities, num_categories)
        
        # Likelihood of observation under each entity
        # Using Hellinger affinity instead of raw likelihood for robustness
        affinities = []
        for i, eid in enumerate(entity_ids):
            # Hellinger affinity = 1 - H² = BC (Bhattacharyya coefficient)
            affinity = self._dirichlet_bhattacharyya(
                self.entities[eid].pseudo_counts,
                observation_likelihoods[i]
            )
            # Weight by entity reliability
            reliability = self._entity_reliability(eid)
            affinities.append(affinity * reliability)
        
        affinities = jnp.array(affinities)
        
        # Add new entity option
        all_affinities = jnp.concatenate([affinities, jnp.array([new_entity_prior])])
        
        # Softmax for responsibilities
        responsibilities = jax.nn.softmax(jnp.log(all_affinities + 1e-10))
        
        return responsibilities[:-1], responsibilities[-1]
    
    def _dirichlet_bhattacharyya(
        self, 
        alpha1: jnp.ndarray, 
        alpha2: jnp.ndarray
    ) -> float:
        """Bhattacharyya coefficient between two Dirichlet distributions."""
        from jax.scipy.special import gammaln
        
        def log_beta(a):
            return jnp.sum(gammaln(a)) - gammaln(jnp.sum(a))
        
        log_bc = log_beta((alpha1 + alpha2) / 2) - 0.5 * (log_beta(alpha1) + log_beta(alpha2))
        return jnp.exp(log_bc)
    
    def _entity_reliability(self, entity_id: int) -> float:
        """
        Compute entity reliability based on:
        - Concentration (higher = more observations = more reliable)
        - Entropy stability (stable = reliable)
        """
        entity = self.entities[entity_id]
        
        # Concentration-based reliability (saturates at max_concentration)
        conc_reliability = jnp.tanh(entity.concentration / self.max_concentration)
        
        # Entropy stability (low variance = reliable)
        if len(entity.association_entropy_history) > 5:
            entropy_std = jnp.std(jnp.array(entity.association_entropy_history[-10:]))
            entropy_reliability = jnp.exp(-entropy_std)
        else:
            entropy_reliability = 0.5  # Uncertain
        
        return float(jnp.sqrt(conc_reliability * entropy_reliability))
    
    def update_entity(
        self,
        entity_id: int,
        observation_counts: jnp.ndarray,
        responsibility: float,
        timestamp: float
    ):
        """Update entity with weighted observation."""
        entity = self.entities[entity_id]
        
        # Apply temporal decay to existing counts
        time_since_seen = timestamp - entity.last_seen
        decay = self.temporal_decay ** time_since_seen
        decayed_counts = entity.pseudo_counts * decay
        
        # Add new observation weighted by responsibility
        weighted_counts = observation_counts * responsibility * self.concentration_scale
        
        # Update
        new_counts = decayed_counts + weighted_counts
        
        # Concentration regulation: keep within bounds
        total = jnp.sum(new_counts)
        if total > self.max_concentration:
            new_counts = new_counts * (self.max_concentration / total)
        elif total < self.min_concentration:
            new_counts = new_counts * (self.min_concentration / total)
        
        entity.pseudo_counts = new_counts
        entity.last_seen = timestamp
        
        # Track history
        entity.concentration_history.append(entity.concentration)
        entity.association_entropy_history.append(entity.entropy)
    
    def adapt_concentration_scale(self):
        """
        Globally adapt concentration scale based on system-wide entropy.
        
        If average entropy is too high (associations too uncertain),
        increase concentration scale to make updates more impactful.
        If too low (over-confident), decrease scale.
        """
        if not self.entities:
            return
        
        # Compute average entropy across entities
        entropies = [e.entropy for e in self.entities.values()]
        avg_entropy = jnp.mean(jnp.array(entropies))
        self.global_entropy_history.append(float(avg_entropy))
        
        # Adapt concentration scale
        entropy_error = avg_entropy - self.target_entropy
        
        # PI controller (simplified)
        kp = 0.1
        adjustment = -kp * entropy_error
        
        self.concentration_scale *= jnp.exp(adjustment)
        self.concentration_scale = jnp.clip(self.concentration_scale, 0.1, 10.0)
    
    def prune_stale_entities(self, current_time: float, max_age: float = 10.0):
        """Remove entities not seen recently."""
        to_remove = [
            eid for eid, entity in self.entities.items()
            if current_time - entity.last_seen > max_age
        ]
        for eid in to_remove:
            del self.entities[eid]
    
    def get_diagnostics(self) -> dict:
        """Get diagnostic information."""
        return {
            'num_entities': len(self.entities),
            'concentration_scale': self.concentration_scale,
            'avg_entropy': jnp.mean(jnp.array(self.global_entropy_history[-10:])) if self.global_entropy_history else 0.0,
            'target_entropy': self.target_entropy,
            'entities': {
                eid: {
                    'concentration': e.concentration,
                    'entropy': e.entropy,
                    'reliability': self._entity_reliability(eid)
                }
                for eid, e in self.entities.items()
            }
        }
```

---

## 4. Adaptive Loop Closure Confidence

### 4.1 Mathematical Foundation

Loop closures can be false positives. Instead of hard χ² gating, use Hellinger distance to softly weight loop confidence:

$$w_{\text{loop}} = \exp\left(-\lambda \cdot H^2(p_{\text{loop}}, p_{\text{current}})\right)$$

where $H^2$ is the squared Hellinger distance between the loop constraint distribution and the current state estimate.

**Adaptive λ:** Track false positive rate and adjust λ accordingly:
- Many false positives → increase λ (stricter gating)
- Few false positives → decrease λ (accept more loops)

### 4.2 Application: Adaptive Loop Gating

```python
# Enhancements to loop_processor.py

import jax.numpy as jnp
from dataclasses import dataclass
from typing import List, Tuple

@dataclass
class LoopCandidate:
    """A candidate loop closure."""
    from_anchor: int
    to_anchor: int
    relative_pose: jnp.ndarray      # 6D (translation, rotation)
    information: jnp.ndarray         # 6x6 precision matrix
    descriptor_distance: float       # Visual/geometric similarity
    timestamp: float


class AdaptiveLoopProcessor:
    """
    Adaptive loop closure processing with:
    1. Hellinger-based soft gating
    2. Adaptive threshold based on historical performance
    3. Consistency checking against recent trajectory
    """
    
    def __init__(
        self,
        initial_lambda: float = 5.0,
        lambda_adaptation_rate: float = 0.01,
        consistency_window: int = 10,
        min_lambda: float = 1.0,
        max_lambda: float = 20.0
    ):
        self.lambda_gate = initial_lambda
        self.lambda_rate = lambda_adaptation_rate
        self.consistency_window = consistency_window
        self.min_lambda = min_lambda
        self.max_lambda = max_lambda
        
        # History tracking
        self.accepted_loops: List[Tuple[LoopCandidate, float]] = []  # (loop, weight)
        self.rejected_loops: List[LoopCandidate] = []
        self.validation_results: List[bool] = []  # True = validated, False = false positive
        
        # Adaptive state
        self.false_positive_rate: float = 0.1  # Prior estimate
        self.acceptance_rate: float = 0.5
    
    def compute_loop_weight(
        self,
        loop: LoopCandidate,
        current_state: GaussianInfo,
        anchor_states: dict[int, GaussianInfo]
    ) -> float:
        """
        Compute adaptive weight for a loop closure.
        
        Weight based on:
        1. Hellinger distance from current estimate
        2. Consistency with recent trajectory
        3. Descriptor quality
        """
        # Get states at loop anchors
        from_state = anchor_states[loop.from_anchor]
        to_state = anchor_states[loop.to_anchor]
        
        # Predicted relative pose from current estimates
        predicted_relative = self._compute_relative(from_state, to_state)
        
        # Loop constraint as Gaussian
        loop_gaussian = GaussianInfo.from_pose_and_info(
            loop.relative_pose, loop.information
        )
        predicted_gaussian = GaussianInfo.from_pose_and_info(
            predicted_relative, self._estimate_relative_info(from_state, to_state)
        )
        
        # Hellinger distance
        h2 = self._gaussian_hellinger_squared(loop_gaussian, predicted_gaussian)
        
        # Base weight from Hellinger
        hellinger_weight = jnp.exp(-self.lambda_gate * h2)
        
        # Descriptor quality factor
        descriptor_weight = jnp.exp(-loop.descriptor_distance)
        
        # Trajectory consistency factor
        consistency_weight = self._trajectory_consistency(loop, anchor_states)
        
        # Combined weight (geometric mean)
        weight = (hellinger_weight * descriptor_weight * consistency_weight) ** (1/3)
        
        return float(weight)
    
    def _gaussian_hellinger_squared(
        self,
        g1: GaussianInfo,
        g2: GaussianInfo
    ) -> float:
        """Squared Hellinger distance between Gaussians."""
        mu1, cov1 = g1.mean, g1.covariance
        mu2, cov2 = g2.mean, g2.covariance
        
        cov_avg = (cov1 + cov2) / 2
        
        # BC = det(Σ₁)^(1/4) det(Σ₂)^(1/4) / det(Σ_avg)^(1/2) × exp(-1/8 δμᵀ Σ_avg⁻¹ δμ)
        det1 = jnp.linalg.det(cov1)
        det2 = jnp.linalg.det(cov2)
        det_avg = jnp.linalg.det(cov_avg)
        
        delta_mu = mu1 - mu2
        mahal = delta_mu @ jnp.linalg.solve(cov_avg, delta_mu)
        
        log_bc = 0.25 * jnp.log(det1) + 0.25 * jnp.log(det2) - 0.5 * jnp.log(det_avg) - 0.125 * mahal
        
        return 1.0 - jnp.exp(log_bc)
    
    def _trajectory_consistency(
        self,
        loop: LoopCandidate,
        anchor_states: dict[int, GaussianInfo]
    ) -> float:
        """Check if loop is consistent with recent trajectory shape."""
        # Get recent anchors
        recent_anchors = sorted(anchor_states.keys())[-self.consistency_window:]
        
        if len(recent_anchors) < 3:
            return 1.0  # Not enough data
        
        # Compute trajectory curvature/smoothness
        positions = jnp.stack([anchor_states[a].mean[:3] for a in recent_anchors])
        
        # Check if loop closure point lies roughly on trajectory extension
        # (simplified: check distance from trajectory plane)
        if loop.to_anchor in anchor_states:
            loop_pos = anchor_states[loop.to_anchor].mean[:3]
            
            # Fit plane to recent trajectory
            centroid = jnp.mean(positions, axis=0)
            centered = positions - centroid
            _, _, vh = jnp.linalg.svd(centered)
            normal = vh[-1]  # Smallest singular vector = normal to best-fit plane
            
            # Distance from loop point to plane
            dist_to_plane = jnp.abs(jnp.dot(loop_pos - centroid, normal))
            trajectory_scale = jnp.std(jnp.linalg.norm(centered, axis=1))
            
            # Consistency: loop point should be within a few trajectory scales
            consistency = jnp.exp(-dist_to_plane / (3 * trajectory_scale + 1e-6))
            return float(consistency)
        
        return 1.0
    
    def process_loop(
        self,
        loop: LoopCandidate,
        current_state: GaussianInfo,
        anchor_states: dict[int, GaussianInfo],
        accept_threshold: float = 0.1
    ) -> Tuple[bool, float]:
        """
        Process a loop candidate.
        
        Returns:
            accepted: Whether loop was accepted
            weight: Weight to use if accepted
        """
        weight = self.compute_loop_weight(loop, current_state, anchor_states)
        
        accepted = weight > accept_threshold
        
        if accepted:
            self.accepted_loops.append((loop, weight))
        else:
            self.rejected_loops.append(loop)
        
        self._update_acceptance_rate()
        
        return accepted, weight
    
    def validate_loop(self, loop: LoopCandidate, was_correct: bool):
        """
        Provide ground truth feedback for a processed loop.
        Call this after external validation (e.g., pose graph converged well).
        """
        self.validation_results.append(was_correct)
        
        # Update false positive rate estimate
        if len(self.validation_results) > 10:
            recent = self.validation_results[-50:]
            self.false_positive_rate = 1.0 - sum(recent) / len(recent)
        
        # Adapt lambda based on false positive rate
        self._adapt_lambda()
    
    def _adapt_lambda(self):
        """Adapt gating threshold based on performance."""
        target_fpr = 0.05  # Target 5% false positive rate
        
        error = self.false_positive_rate - target_fpr
        
        # Increase lambda if too many false positives
        # Decrease if too few (being too conservative)
        delta_lambda = self.lambda_rate * error * self.lambda_gate
        
        self.lambda_gate = jnp.clip(
            self.lambda_gate + delta_lambda,
            self.min_lambda,
            self.max_lambda
        )
    
    def _update_acceptance_rate(self):
        """Track acceptance rate."""
        total = len(self.accepted_loops) + len(self.rejected_loops)
        if total > 0:
            self.acceptance_rate = len(self.accepted_loops) / total
    
    def get_diagnostics(self) -> dict:
        """Get diagnostic information."""
        return {
            'lambda_gate': self.lambda_gate,
            'false_positive_rate': self.false_positive_rate,
            'acceptance_rate': self.acceptance_rate,
            'total_accepted': len(self.accepted_loops),
            'total_rejected': len(self.rejected_loops),
            'recent_weights': [w for _, w in self.accepted_loops[-10:]]
        }
```

---

## 5. Adaptive Frobenius Correction Strength

### 5.1 Mathematical Foundation

The Frobenius cubic correction compensates for linearization error:
$$\theta_{\text{corrected}} = \theta_{\text{linear}} + \beta \cdot (\delta\theta \circ \delta\theta)$$

The correction strength $\beta$ should adapt based on:
- **Observed linearization error:** If predictions consistently over/undershoot, adjust β
- **Dynamics level:** Higher dynamics → larger corrections needed
- **Convergence behavior:** If optimization struggles, correction may be too strong/weak

### 5.2 Application: Adaptive Cubic Correction

```python
# New file: backend/adaptive_frobenius.py

import jax.numpy as jnp
from dataclasses import dataclass
from typing import List, Tuple

@dataclass  
class FrobeniusCorrector:
    """
    Adaptive Frobenius cubic correction for linearization error.
    """
    
    # Correction strength (0 = no correction, 1 = full geometric correction)
    beta: float = 0.5
    
    # Adaptation parameters
    adaptation_rate: float = 0.01
    min_beta: float = 0.0
    max_beta: float = 2.0
    
    # History for adaptation
    prediction_errors: List[float] = None
    correction_magnitudes: List[float] = None
    
    def __post_init__(self):
        if self.prediction_errors is None:
            self.prediction_errors = []
        if self.correction_magnitudes is None:
            self.correction_magnitudes = []
    
    def compute_correction(
        self,
        delta_theta: jnp.ndarray,
        metric: jnp.ndarray,
        cubic_tensor: jnp.ndarray
    ) -> jnp.ndarray:
        """
        Compute Frobenius cubic correction.
        
        The induced multiplication satisfies: g(u ∘ v, w) = C(u, v, w)
        So: u ∘ v = g⁻¹ C(u, v, ·)
        """
        # Compute δθ ∘ δθ
        # C is (dim, dim, dim) tensor
        metric_inv = jnp.linalg.inv(metric)
        
        # Contract: (g⁻¹)^{ik} C_{kjl} δθ^j δθ^l
        circ_product = jnp.einsum(
            'ik,kjl,j,l->i',
            metric_inv, cubic_tensor, delta_theta, delta_theta
        )
        
        correction = self.beta * 0.5 * circ_product
        
        # Track correction magnitude
        self.correction_magnitudes.append(float(jnp.linalg.norm(correction)))
        
        return delta_theta + correction
    
    def update_from_prediction_error(
        self,
        predicted: jnp.ndarray,
        actual: jnp.ndarray,
        correction_applied: jnp.ndarray
    ):
        """
        Update beta based on observed prediction error.
        
        If correction was in the right direction but too small/large,
        adjust beta accordingly.
        """
        error = actual - predicted
        error_norm = jnp.linalg.norm(error)
        self.prediction_errors.append(float(error_norm))
        
        if len(self.prediction_errors) < 10:
            return  # Need more data
        
        # Check if correction direction correlates with error
        if jnp.linalg.norm(correction_applied) > 1e-10:
            # Correlation between correction and error
            # Positive = correction helped, negative = correction hurt
            correlation = jnp.dot(correction_applied, error) / (
                jnp.linalg.norm(correction_applied) * error_norm + 1e-10
            )
            
            # If correction is in wrong direction, reduce beta
            # If correction is right direction but error still large, increase beta
            if correlation < 0:
                # Wrong direction - reduce
                self.beta *= (1 - self.adaptation_rate)
            else:
                # Right direction - check magnitude
                recent_errors = jnp.array(self.prediction_errors[-10:])
                error_trend = jnp.mean(recent_errors[-5:]) - jnp.mean(recent_errors[:5])
                
                if error_trend > 0:
                    # Errors increasing - need more correction
                    self.beta *= (1 + self.adaptation_rate)
                elif error_trend < 0:
                    # Errors decreasing - current beta is good or slightly reduce
                    pass
        
        # Clamp
        self.beta = jnp.clip(self.beta, self.min_beta, self.max_beta)
    
    def estimate_cubic_tensor_online(
        self,
        log_partition_samples: List[Tuple[jnp.ndarray, float]],  # (theta, psi(theta))
        current_theta: jnp.ndarray
    ) -> jnp.ndarray:
        """
        Estimate the cubic tensor C = ∇³ψ from samples when analytic form unavailable.
        
        Uses finite differences on collected samples near current operating point.
        """
        if len(log_partition_samples) < 20:
            # Return zero tensor if insufficient samples
            dim = current_theta.shape[0]
            return jnp.zeros((dim, dim, dim))
        
        # Find samples near current theta
        thetas = jnp.stack([s[0] for s in log_partition_samples])
        psis = jnp.array([s[1] for s in log_partition_samples])
        
        dists = jnp.linalg.norm(thetas - current_theta, axis=1)
        nearby_idx = jnp.argsort(dists)[:20]
        
        nearby_thetas = thetas[nearby_idx]
        nearby_psis = psis[nearby_idx]
        
        # Fit local cubic polynomial and extract third derivatives
        # (Simplified: use finite difference approximation)
        dim = current_theta.shape[0]
        cubic = jnp.zeros((dim, dim, dim))
        
        h = jnp.mean(dists[nearby_idx]) / 2  # Step size
        
        for i in range(dim):
            for j in range(dim):
                for k in range(dim):
                    # Third partial derivative approximation
                    # ∂³ψ/∂θᵢ∂θⱼ∂θₖ
                    ei = jnp.zeros(dim).at[i].set(h)
                    ej = jnp.zeros(dim).at[j].set(h)
                    ek = jnp.zeros(dim).at[k].set(h)
                    
                    # Find nearest samples in each direction and use interpolation
                    # (This is a simplified approximation)
                    cubic = cubic.at[i,j,k].set(
                        self._estimate_third_derivative(
                            nearby_thetas, nearby_psis, current_theta, i, j, k, h
                        )
                    )
        
        return cubic
    
    def _estimate_third_derivative(
        self,
        thetas: jnp.ndarray,
        psis: jnp.ndarray,
        center: jnp.ndarray,
        i: int, j: int, k: int,
        h: float
    ) -> float:
        """Estimate ∂³ψ/∂θᵢ∂θⱼ∂θₖ using nearby samples."""
        # Use weighted local regression
        # (Placeholder - in practice use proper finite difference or autodiff)
        return 0.0  # Conservative default
    
    def get_diagnostics(self) -> dict:
        """Get diagnostic information."""
        return {
            'beta': self.beta,
            'mean_correction_magnitude': jnp.mean(jnp.array(self.correction_magnitudes[-50:])) if self.correction_magnitudes else 0.0,
            'mean_prediction_error': jnp.mean(jnp.array(self.prediction_errors[-50:])) if self.prediction_errors else 0.0,
            'error_trend': (
                jnp.mean(jnp.array(self.prediction_errors[-10:])) - 
                jnp.mean(jnp.array(self.prediction_errors[-20:-10]))
            ) if len(self.prediction_errors) > 20 else 0.0
        }
```

---

## 6. System-Wide Adaptive Coordinator

### 6.1 Bringing It All Together

```python
# New file: backend/adaptive_coordinator.py

import jax.numpy as jnp
from dataclasses import dataclass
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

@dataclass
class SystemHealth:
    """Overall system health metrics."""
    imu_health: float = 1.0
    odom_health: float = 1.0
    loop_health: float = 1.0
    association_health: float = 1.0
    overall_health: float = 1.0
    
    # Alerts
    alerts: list = None
    
    def __post_init__(self):
        if self.alerts is None:
            self.alerts = []


class AdaptiveCoordinator:
    """
    Coordinates all adaptive subsystems and provides system-wide health monitoring.
    
    Subsystems:
    - AdaptiveIMUNoiseModel: IMU noise adaptation
    - AdaptiveFusionEngine: Sensor weighting
    - AdaptiveDirichletRouter: Entity association
    - AdaptiveLoopProcessor: Loop closure gating
    - FrobeniusCorrector: Linearization correction
    """
    
    def __init__(
        self,
        imu_model: 'AdaptiveIMUNoiseModel',
        fusion_engine: 'AdaptiveFusionEngine',
        dirichlet_router: 'AdaptiveDirichletRouter',
        loop_processor: 'AdaptiveLoopProcessor',
        frobenius_corrector: 'FrobeniusCorrector'
    ):
        self.imu = imu_model
        self.fusion = fusion_engine
        self.dirichlet = dirichlet_router
        self.loops = loop_processor
        self.frobenius = frobenius_corrector
        
        # Cross-system adaptation
        self.health_history: list = []
        self.degradation_mode: bool = False
    
    def compute_system_health(self) -> SystemHealth:
        """Compute overall system health from all subsystems."""
        health = SystemHealth()
        alerts = []
        
        # IMU health from anomaly detection
        imu_anomalies = self.imu.detect_anomaly()
        imu_severities = [a['severity'] for a in imu_anomalies.values() if a.get('anomaly')]
        if imu_severities:
            health.imu_health = float(jnp.exp(-jnp.mean(jnp.array(imu_severities))))
            alerts.append(f"IMU anomaly detected, severity={max(imu_severities):.2f}")
        
        # Fusion health from sensor reliabilities
        fusion_diag = self.fusion.get_diagnostics()
        reliabilities = [s['reliability'] for s in fusion_diag.values()]
        if reliabilities:
            health.odom_health = float(jnp.mean(jnp.array(reliabilities)))
            if health.odom_health < 0.5:
                alerts.append(f"Low sensor reliability: {health.odom_health:.2f}")
        
        # Loop health from acceptance/FP rates
        loop_diag = self.loops.get_diagnostics()
        if loop_diag['false_positive_rate'] > 0.2:
            health.loop_health = 1.0 - loop_diag['false_positive_rate']
            alerts.append(f"High loop FP rate: {loop_diag['false_positive_rate']:.2f}")
        else:
            health.loop_health = 1.0
        
        # Association health from entropy
        dirichlet_diag = self.dirichlet.get_diagnostics()
        entropy_ratio = dirichlet_diag['avg_entropy'] / (dirichlet_diag['target_entropy'] + 1e-6)
        # Health is best when entropy is near target
        health.association_health = float(jnp.exp(-abs(entropy_ratio - 1.0)))
        if entropy_ratio > 2.0:
            alerts.append(f"Association entropy too high: {entropy_ratio:.2f}x target")
        elif entropy_ratio < 0.3:
            alerts.append(f"Association over-confident: {entropy_ratio:.2f}x target")
        
        # Overall health (geometric mean)
        health.overall_health = float(
            (health.imu_health * health.odom_health * 
             health.loop_health * health.association_health) ** 0.25
        )
        
        health.alerts = alerts
        self.health_history.append(health.overall_health)
        
        # Check for degradation mode
        if len(self.health_history) > 10:
            recent_health = jnp.array(self.health_history[-10:])
            if jnp.mean(recent_health) < 0.5:
                self.degradation_mode = True
                logger.warning("Entering degradation mode due to low system health")
        
        return health
    
    def cross_system_adaptation(self):
        """
        Perform cross-system adaptations based on overall health.
        
        When one subsystem degrades, others should compensate:
        - Bad IMU → increase odom weight, loosen loop thresholds
        - Bad loops → tighten association, increase IMU trust
        - Bad association → rely more on geometric constraints
        """
        health = self.compute_system_health()
        
        if health.imu_health < 0.5:
            # IMU degraded: increase reliance on other sensors
            logger.info("Adapting to IMU degradation")
            
            # Reduce IMU weight in fusion
            if 'imu' in self.fusion.sensors:
                self.fusion.sensors['imu'].reliability *= 0.5
            
            # Accept more loops to compensate
            self.loops.lambda_gate *= 0.8
        
        if health.loop_health < 0.5:
            # Loops unreliable: tighten everything else
            logger.info("Adapting to loop degradation")
            
            # Tighten loop acceptance
            self.loops.lambda_gate *= 1.2
            
            # Rely more on odometry
            for sid, model in self.fusion.sensors.items():
                if 'odom' in sid.lower():
                    model.reliability = min(1.0, model.reliability * 1.1)
        
        if health.association_health < 0.5:
            # Association confused: reset concentration scales
            logger.info("Adapting to association degradation")
            self.dirichlet.concentration_scale = 1.0
            self.dirichlet.adapt_concentration_scale()
        
        if self.degradation_mode and health.overall_health > 0.7:
            # Recovered from degradation
            self.degradation_mode = False
            logger.info("Exiting degradation mode - system recovered")
    
    def get_all_diagnostics(self) -> Dict[str, Any]:
        """Get diagnostics from all subsystems."""
        return {
            'health': self.compute_system_health().__dict__,
            'degradation_mode': self.degradation_mode,
            'imu': self.imu.get_current_noise_params(),
            'fusion': self.fusion.get_diagnostics(),
            'dirichlet': self.dirichlet.get_diagnostics(),
            'loops': self.loops.get_diagnostics(),
            'frobenius': self.frobenius.get_diagnostics()
        }
    
    def step(self):
        """Called each iteration to perform adaptation."""
        self.cross_system_adaptation()
        self.dirichlet.adapt_concentration_scale()


# Factory function for easy setup
def create_adaptive_system(
    imu_config: dict = None,
    sensor_configs: dict = None,
    num_semantic_categories: int = 10
) -> AdaptiveCoordinator:
    """Create a fully configured adaptive system."""
    
    # IMU model
    imu_config = imu_config or {}
    imu_model = create_default_imu_adaptive_model(**imu_config)
    
    # Fusion engine
    fusion = AdaptiveFusionEngine()
    if sensor_configs:
        for name, config in sensor_configs.items():
            fusion.register_sensor(name, config['noise_cov'], config.get('confidence', 10.0))
    
    # Dirichlet router
    dirichlet = AdaptiveDirichletRouter(num_categories=num_semantic_categories)
    
    # Loop processor
    loops = AdaptiveLoopProcessor()
    
    # Frobenius corrector
    frobenius = FrobeniusCorrector()
    
    return AdaptiveCoordinator(
        imu_model=imu_model,
        fusion_engine=fusion,
        dirichlet_router=dirichlet,
        loop_processor=loops,
        frobenius_corrector=frobenius
    )
```

---

## 7. Diagnostic Dashboard Integration

For ROS/Foxglove visualization:

```python
# New file: backend/adaptive_diagnostics.py

from std_msgs.msg import String
import json

class AdaptiveDiagnosticsPublisher:
    """Publishes adaptive system diagnostics for Foxglove."""
    
    def __init__(self, node, coordinator: AdaptiveCoordinator):
        self.coordinator = coordinator
        self.pub = node.create_publisher(String, '/cdwm/adaptive_diagnostics', 10)
        
        # Create timer for periodic publishing
        self.timer = node.create_timer(1.0, self.publish_diagnostics)
    
    def publish_diagnostics(self):
        """Publish all diagnostics as JSON."""
        diag = self.coordinator.get_all_diagnostics()
        
        # Convert numpy arrays to lists for JSON
        def convert(obj):
            if hasattr(obj, 'tolist'):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert(v) for v in obj]
            return obj
        
        msg = String()
        msg.data = json.dumps(convert(diag), indent=2)
        self.pub.publish(msg)
```

---

## 7. Monge-Ampère Transport for Dynamic Map Adaptation

### 7.1 Mathematical Foundation

Monge-Ampère equations govern optimal transport between probability densities, enabling self-adaptive warping of maps or covariances in dynamic environments. This is ideal for handling edge cases like scene flow (e.g., moving objects in M3DGR bag) or merging degraded submaps.

**Key Equation (Monge-Ampère):**

For densities $f$ (source map) and $g$ (target map), solve for potential $\phi$:

$$\det(\nabla^2 \phi) = \frac{f}{g} \cdot \exp(-\text{energy})$$

The transport map is $\nabla \phi$, minimizing Wasserstein distance $W_2(f, g)$.

**Connection to Wishart/Info-Geo:**
- **Transport Wishart precision matrices equivariantly:** If $\Lambda \sim W_p(n, S)$, then transported $\Lambda' = A \Lambda A^T$ (from additivity/equivariance).
- **Trigger via Hellinger:** If $H^2(f, g) > \tau_H$ (e.g., 0.3), initiate transport to adapt to dynamics.
- **Legendre Duality Tie:** Dual potentials $\phi^*$ enable closed-form approximations for high-dim maps (e.g., via Bregman projections on covariances).

**Cramér-Rao Bound for Transport Error (from Hellinger Priors):**

$$\inf R(\hat{\phi}) \geq n^{-2/\alpha} C(\alpha) \int J^{-1}(\theta) \pi_H(\theta) \, d\theta$$

where $\pi_H \propto \sqrt{\det J}$ (Hellinger matrix) bounds variance in adapted maps.

### 7.2 Application: Adaptive Scene Flow and Map Warping

**Use Cases:**
- In dynamic scenes, transport static map densities while filtering movers (downweight high-$W_2$ regions)
- Handle slippage/collisions by warping IMU-derived flows to RGB-D frames
- Multi-robot map merging (transport submaps associatively)

**Trigger Logic:**
- Compute $W_2 \approx \int \phi \, df + \phi^* \, dg$
- Adapt if $W_2 >$ threshold or Hellinger divergence exceeds $\tau_H$

```python
# New file: backend/adaptive_transport.py

import jax
import jax.numpy as jnp
from typing import Tuple, Optional
from dataclasses import dataclass
from .adaptive_noise import WishartPrior

@jax.jit
def sinkhorn_transport(
    source: jnp.ndarray, 
    target: jnp.ndarray,
    cost_matrix: jnp.ndarray,
    epsilon: float = 0.01,
    max_iters: int = 100
) -> Tuple[jnp.ndarray, float]:
    """
    Sinkhorn algorithm for entropy-regularized optimal transport.
    
    Approximates Monge-Ampère solution via iterative scaling.
    """
    n, m = source.shape[0], target.shape[0]
    
    # Normalize to probability distributions
    p = source / (jnp.sum(source) + 1e-10)
    q = target / (jnp.sum(target) + 1e-10)
    
    # Gibbs kernel
    K = jnp.exp(-cost_matrix / epsilon)
    
    # Initialize scaling vectors
    u = jnp.ones(n)
    v = jnp.ones(m)
    
    # Sinkhorn iterations
    def sinkhorn_step(carry, _):
        u, v = carry
        u_new = p / (K @ v + 1e-10)
        v_new = q / (K.T @ u_new + 1e-10)
        return (u_new, v_new), None
    
    (u, v), _ = jax.lax.scan(sinkhorn_step, (u, v), None, length=max_iters)
    
    # Transport plan
    transport_plan = jnp.diag(u) @ K @ jnp.diag(v)
    
    # Wasserstein cost
    w2_cost = jnp.sum(transport_plan * cost_matrix)
    
    return transport_plan, w2_cost


def compute_cost_matrix(
    source_points: jnp.ndarray,  # (N, D) source positions
    target_points: jnp.ndarray   # (M, D) target positions
) -> jnp.ndarray:
    """Squared Euclidean cost matrix."""
    diff = source_points[:, None, :] - target_points[None, :, :]
    return jnp.sum(diff ** 2, axis=-1)


@dataclass
class AdaptiveTransport:
    """
    Monge-Ampère optimal transport adapter for dynamic maps.
    
    Uses Hellinger trigger to decide when transport is needed,
    then applies Sinkhorn OT to warp source → target.
    """
    
    wishart_prior: WishartPrior
    hellinger_tau: float = 0.3      # Trigger threshold
    w2_tau: float = 1.0             # Wasserstein alarm threshold
    epsilon: float = 0.01           # Sinkhorn regularization
    
    # State
    last_cost: float = 0.0
    last_hellinger: float = 0.0
    transport_history: list = None
    
    def __post_init__(self):
        if self.transport_history is None:
            self.transport_history = []
    
    def compute_hellinger(
        self, 
        source: jnp.ndarray, 
        target: jnp.ndarray
    ) -> float:
        """Squared Hellinger distance between density arrays."""
        # Normalize
        p = source / (jnp.sum(source) + 1e-10)
        q = target / (jnp.sum(target) + 1e-10)
        
        # H² = ½ Σ (√p - √q)²
        h2 = 0.5 * jnp.sum((jnp.sqrt(p) - jnp.sqrt(q)) ** 2)
        return float(h2)
    
    def should_transport(self, source: jnp.ndarray, target: jnp.ndarray) -> bool:
        """Check if transport is needed based on Hellinger trigger."""
        self.last_hellinger = self.compute_hellinger(source, target)
        return self.last_hellinger > self.hellinger_tau
    
    def transport(
        self,
        source_density: jnp.ndarray,   # (N,) density values
        target_density: jnp.ndarray,   # (M,) density values
        source_points: jnp.ndarray,    # (N, D) spatial positions
        target_points: jnp.ndarray     # (M, D) spatial positions
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Perform optimal transport from source to target.
        
        Returns:
            transported_density: Source density warped to target support
            transport_plan: (N, M) coupling matrix
        """
        # Compute cost matrix
        cost_matrix = compute_cost_matrix(source_points, target_points)
        
        # Sinkhorn OT
        transport_plan, w2_cost = sinkhorn_transport(
            source_density, target_density, cost_matrix, self.epsilon
        )
        
        self.last_cost = float(w2_cost)
        self.transport_history.append({
            'hellinger': self.last_hellinger,
            'w2_cost': self.last_cost,
            'triggered': True
        })
        
        # Transported density: push forward source through plan
        transported = transport_plan.T @ source_density
        
        return transported, transport_plan
    
    def update(
        self,
        source_density: jnp.ndarray,
        target_density: jnp.ndarray,
        source_points: jnp.ndarray,
        target_points: jnp.ndarray
    ) -> jnp.ndarray:
        """
        Conditionally transport source to target if Hellinger trigger fires.
        """
        if not self.should_transport(source_density, target_density):
            self.transport_history.append({
                'hellinger': self.last_hellinger,
                'w2_cost': 0.0,
                'triggered': False
            })
            return source_density
        
        transported, _ = self.transport(
            source_density, target_density, source_points, target_points
        )
        
        return transported
    
    def transport_wishart_equivariant(
        self,
        transform_matrix: jnp.ndarray  # Linear transform from transport
    ):
        """
        Update Wishart prior equivariantly under transport.
        
        If Λ ~ W_p(n, S), then AΛA^T ~ W_q(n, ASA^T).
        """
        A = transform_matrix
        self.wishart_prior = WishartPrior(
            n=self.wishart_prior.n,
            S=A @ self.wishart_prior.S @ A.T
        )
    
    def get_diagnostics(self) -> dict:
        """Get transport diagnostics."""
        recent = self.transport_history[-20:] if self.transport_history else []
        
        return {
            'last_hellinger': self.last_hellinger,
            'last_w2_cost': self.last_cost,
            'hellinger_threshold': self.hellinger_tau,
            'transport_rate': sum(1 for t in recent if t['triggered']) / max(len(recent), 1),
            'avg_w2_when_triggered': jnp.mean(jnp.array([
                t['w2_cost'] for t in recent if t['triggered']
            ])) if any(t['triggered'] for t in recent) else 0.0,
            'alarm': self.last_cost > self.w2_tau
        }


class SceneFlowAdapter(AdaptiveTransport):
    """
    Specialized transport for scene flow in dynamic environments.
    
    Handles:
    - Moving objects (high local W2)
    - Static background (low local W2)
    - Filtering/downweighting dynamic regions
    """
    
    def __init__(
        self,
        wishart_prior: WishartPrior,
        grid_resolution: Tuple[int, int, int] = (64, 64, 32),
        dynamic_threshold: float = 0.5
    ):
        super().__init__(wishart_prior)
        self.grid_resolution = grid_resolution
        self.dynamic_threshold = dynamic_threshold
        self.dynamic_mask = None
    
    def compute_local_transport_cost(
        self,
        source_grid: jnp.ndarray,   # (X, Y, Z) voxel densities
        target_grid: jnp.ndarray,
        window_size: int = 4
    ) -> jnp.ndarray:
        """
        Compute local W2 cost in sliding windows to identify dynamic regions.
        """
        X, Y, Z = source_grid.shape
        local_costs = jnp.zeros_like(source_grid)
        
        # Simplified: use local Hellinger as proxy for W2
        # (Full local OT is expensive; Hellinger is fast and correlated)
        for dx in range(-window_size//2, window_size//2 + 1):
            for dy in range(-window_size//2, window_size//2 + 1):
                for dz in range(-window_size//2, window_size//2 + 1):
                    shifted_target = jnp.roll(jnp.roll(jnp.roll(
                        target_grid, dx, axis=0), dy, axis=1), dz, axis=2)
                    
                    local_h2 = (jnp.sqrt(source_grid + 1e-10) - 
                               jnp.sqrt(shifted_target + 1e-10)) ** 2
                    local_costs += local_h2
        
        return local_costs / (window_size ** 3)
    
    def identify_dynamic_regions(
        self,
        source_grid: jnp.ndarray,
        target_grid: jnp.ndarray
    ) -> jnp.ndarray:
        """
        Return mask of dynamic (moving) regions.
        """
        local_costs = self.compute_local_transport_cost(source_grid, target_grid)
        self.dynamic_mask = local_costs > self.dynamic_threshold
        return self.dynamic_mask
    
    def filter_dynamic_for_slam(
        self,
        point_cloud: jnp.ndarray,     # (N, 3) points
        densities: jnp.ndarray,        # (N,) density/weight per point
        source_grid: jnp.ndarray,
        target_grid: jnp.ndarray
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Downweight points in dynamic regions for SLAM.
        
        Returns filtered points and weights.
        """
        # Get dynamic mask
        dynamic_mask = self.identify_dynamic_regions(source_grid, target_grid)
        
        # Map points to grid indices
        # (Assumes points are in grid coordinates; adjust as needed)
        grid_indices = jnp.clip(point_cloud.astype(int), 0, 
                                jnp.array(self.grid_resolution) - 1)
        
        # Check if each point is in dynamic region
        is_dynamic = dynamic_mask[
            grid_indices[:, 0], 
            grid_indices[:, 1], 
            grid_indices[:, 2]
        ]
        
        # Downweight dynamic points (don't remove, just reduce influence)
        weights = jnp.where(is_dynamic, densities * 0.1, densities)
        
        return point_cloud, weights
    
    def get_diagnostics(self) -> dict:
        base = super().get_diagnostics()
        base.update({
            'dynamic_fraction': float(jnp.mean(self.dynamic_mask)) if self.dynamic_mask is not None else 0.0,
            'grid_resolution': self.grid_resolution
        })
        return base
```

### 7.3 Integration with Coordinator

Add to `AdaptiveCoordinator.__init__`:

```python
# In adaptive_coordinator.py

from .adaptive_transport import AdaptiveTransport, SceneFlowAdapter

class AdaptiveCoordinator:
    def __init__(self, ...):
        # ... existing init ...
        
        # Add transport adapter
        self.transport = AdaptiveTransport(
            wishart_prior=self.imu.accel_noise,  # Share prior for equivariance
            hellinger_tau=0.3,
            w2_tau=1.0
        )
        
        # Scene flow for dynamic filtering (optional, for dense mapping)
        self.scene_flow = SceneFlowAdapter(
            wishart_prior=self.imu.accel_noise,
            grid_resolution=(64, 64, 32)
        )
    
    def adapt_map(
        self, 
        source_map: jnp.ndarray, 
        target_map: jnp.ndarray,
        source_points: jnp.ndarray,
        target_points: jnp.ndarray
    ) -> jnp.ndarray:
        """Adapt source map to target via optimal transport if needed."""
        adapted = self.transport.update(
            source_map, target_map, source_points, target_points
        )
        
        # Check for alarm condition
        if self.transport.last_cost > self.transport.w2_tau:
            logger.warning(f"High transport cost: {self.transport.last_cost:.3f}")
            # Reduce map health
            if hasattr(self, 'health'):
                self.health.map_health *= 0.8
        
        return adapted
    
    def filter_dynamic_points(
        self,
        points: jnp.ndarray,
        weights: jnp.ndarray,
        prev_grid: jnp.ndarray,
        curr_grid: jnp.ndarray
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """Filter dynamic points for robust SLAM."""
        return self.scene_flow.filter_dynamic_for_slam(
            points, weights, prev_grid, curr_grid
        )
    
    def get_all_diagnostics(self) -> Dict[str, Any]:
        """Get diagnostics from all subsystems including transport."""
        diag = {
            # ... existing diagnostics ...
            'transport': self.transport.get_diagnostics(),
            'scene_flow': self.scene_flow.get_diagnostics()
        }
        return diag
```

### 7.4 Edge-Case Examples

| Edge Case | Transport Response | Metric to Monitor |
|-----------|-------------------|-------------------|
| **Moving objects** | High local $W_2$ → downweight in SLAM | `dynamic_fraction` |
| **Scene change** | Global Hellinger trigger → full transport | `last_hellinger` vs `hellinger_tau` |
| **Multi-robot merge** | Transport submaps associatively | `w2_cost` between submaps |
| **IMU-induced drift** | Warp accumulated map to RGB-D frame | `last_w2_cost` trend |
| **Slippage/collision** | Sudden $W_2$ spike → alert + adapt | `alarm` flag |

### 7.5 Evaluation Notes

For benchmarking on dynamic sequences (e.g., TUM RGB-D dynamic, M3DGR):

1. **Metric:** Compare ATE/RPE with and without transport adaptation
2. **Target:** <5% drift reduction in dynamic scenes
3. **Diagnostic:** Monitor `transport_rate` — should be low (~10-20%) in mostly-static scenes, higher in dynamic
4. **Ablation:** Disable `SceneFlowAdapter` to measure contribution of dynamic filtering

---

## Summary: Self-Adaptive Components

| Component | Adaptation Mechanism | Key Metric | Benefit |
|-----------|---------------------|------------|---------|
| **IMU Noise** | Wishart conjugate updates | Residual covariance | Handles sensor degradation, temperature drift |
| **Sensor Weights** | Fisher information + reliability | Hellinger divergence | Automatic sensor trust adjustment |
| **Association** | Concentration regulation | Entropy vs target | Robust entity tracking in dynamic scenes |
| **Loop Closure** | Adaptive λ gating | False positive rate | Balances recall vs precision |
| **Frobenius** | β from prediction error | Correction magnitude | Optimal linearization compensation |
| **Cross-System** | Health-based coordination | Overall health score | Graceful degradation, recovery |
| **Dynamic Maps** | Monge-Ampère transport | Wasserstein cost | Handles scene changes, map merging |

All components use **closed-form exponential family updates** where possible, ensuring:
- Computationally tractable (no iterative optimization in adaptation)
- Mathematically principled (conjugate priors, information geometry)
- Robust to edge cases (soft gating, bounded parameters)
